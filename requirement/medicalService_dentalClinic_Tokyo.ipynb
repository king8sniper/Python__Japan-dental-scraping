{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€URLãªã©å„ç¨®è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import necessary library\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import datetime\n",
    "import getpass\n",
    "import jaconv\n",
    "from normalize_japanese_addresses import normalize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import random\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### individual configure\n",
    "SOURCE_NAME = \"medicalInfoService\" \n",
    "BASE_URL = \"https://www.himawari.metro.tokyo.jp/qq13/qqport/tomintop/\"\n",
    "START_URL = \"https://www.himawari.metro.tokyo.jp/qq13/qqport/tomintop/\"\n",
    "WAIT_SEC = 5\n",
    "maxTry = 5\n",
    "dt_now = datetime.datetime.now()\n",
    "page = 1\n",
    "num = 0\n",
    "EXPORT_PATH = r\"shops\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ã‚¦ã‚§ãƒ–ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ç³»ã®é–¢æ•°è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_driver():\n",
    "    # Seleniumç”¨ã®ã‚¦ã‚§ãƒ–ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–ã—ã€ã•ã¾ã–ã¾ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å®‰å®šã—ãŸæœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ã€‚\n",
    "    # Seleniumç”¨ã®Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®šã€‚\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--disable-extensions')  # ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãŸã‚ã«ãƒ–ãƒ©ã‚¦ã‚¶æ‹¡å¼µã‚’ç„¡åŠ¹ã«ã™ã‚‹ã€‚\n",
    "    options.add_argument('--start-maximized')  # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æœ€å¤§åŒ–ã—ãŸã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§é–‹å§‹ã€‚å‚è€ƒ: https://stackoverflow.com/a/26283818/1689770\n",
    "    options.add_argument('--no-sandbox')  # äº’æ›æ€§å‘ä¸Šã®ãŸã‚ã«ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ã‚’ç„¡åŠ¹ã«ã™ã‚‹ã€‚å‚è€ƒ: https://stackoverflow.com/a/50725918/1689770\n",
    "    options.add_argument('--disable-dev-shm-usage')  # ã‚ˆã‚Šå®‰å®šã—ãŸå‹•ä½œã®ãŸã‚ã«ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã€‚å‚è€ƒ: https://stackoverflow.com/a/50725918/1689770\n",
    "\n",
    "    # ä¸»å‡¦ç†\n",
    "    try:\n",
    "        driver_path = ChromeDriverManager().install()\n",
    "        service = Service(executable_path=driver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    except ValueError:\n",
    "        # æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’å–å¾—ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚\n",
    "        url = r'https://googlechromelabs.github.io/chrome-for-testing/last-known-good-versions-with-downloads.json'\n",
    "        response = requests.get(url)\n",
    "        data_dict = response.json()\n",
    "        latest_version = data_dict[\"channels\"][\"Stable\"][\"version\"]\n",
    "\n",
    "        driver_path = ChromeDriverManager(version=latest_version).install()\n",
    "        service = Service(executable_path=driver_path)\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    except PermissionError:  # æš«å®šå‡¦ç† å‚è€ƒ: https://note.com/yuu________/n/n14d97c155e5e\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=Service(f'C:\\\\Users\\\\{USERNAME}\\\\.wdm\\\\drivers\\\\chromedriver\\\\win64\\\\116.0.5845.97\\\\chromedriver.exe'), options=options)\n",
    "        except:\n",
    "            driver = webdriver.Chrome(service=Service(f'C:\\\\Users\\\\{USERNAME}\\\\.wdm\\\\drivers\\\\chromedriver\\\\win64\\\\116.0.5845.96\\\\chromedriver.exe'), options=options)\n",
    "\n",
    "    # ãƒ–ãƒ©ã‚¦ã‚¶ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æœ€å¤§åŒ–ã€‚\n",
    "    driver.maximize_window()\n",
    "    # ã‚¦ã‚§ãƒ–ãƒ‰ãƒ©ã‚¤ãƒã®å¾…æ©Ÿæ™‚é–“ã‚’è¨­å®šã€‚\n",
    "    wait = WebDriverWait(driver, WAIT_SEC)\n",
    "\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ–‡å­—åˆ—æ“ä½œç³»ã®é–¢æ•°è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "def _normalization(arg):\n",
    "    \"\"\"\n",
    "    æ–‡å­—åˆ—ã®æ­£è¦åŒ–ã‚’è¡Œã†å†…éƒ¨é–¢æ•°ã€‚\n",
    "    ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«ã€å…¨è§’ã‚’åŠè§’ã«ã€å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã€ä¸å¯è¦–æ–‡å­—ã‚‚å‰Šé™¤ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›\n",
    "        try:\n",
    "            result = jaconv.hira2kata(arg)\n",
    "        except AttributeError:\n",
    "            result = arg\n",
    "\n",
    "        # å…¨è§’ã‚’åŠè§’ã«å¤‰æ›\n",
    "        try:\n",
    "            result = jaconv.z2h(result, digit=True, ascii=True)\n",
    "        except AttributeError:\n",
    "            result = result\n",
    "\n",
    "        # å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›\n",
    "        try:\n",
    "            result = result.lower()\n",
    "        except AttributeError:\n",
    "            result = result\n",
    "\n",
    "        # ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤\n",
    "        try:\n",
    "            result = _str_clean(result)\n",
    "        except TypeError:\n",
    "            result = result\n",
    "\n",
    "    except:\n",
    "        result = arg\n",
    "\n",
    "    return result\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def normalization(arg):\n",
    "    \"\"\"\n",
    "    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›\n",
    "    _func = np.frompyfunc(_normalization, 1, 1)\n",
    "\n",
    "    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›\n",
    "    _list = np.array(arg, dtype=\"object\")\n",
    "\n",
    "    # çµæœã‚’å–å¾—\n",
    "    result = _func(_list)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›\n",
    "    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else \"error\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def _str_clean(arg):\n",
    "    \"\"\"\n",
    "    æ–‡å­—åˆ—ã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹å†…éƒ¨é–¢æ•°ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        result = arg.strip()\n",
    "    except:\n",
    "        result = arg\n",
    "\n",
    "    try:\n",
    "        result = re.sub(r\"\\r|\\n|\\r\\n|\\u3000|\\t|ã€€| |,\", \" \", result)\n",
    "    except TypeError:\n",
    "        result = result\n",
    "\n",
    "    return result\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def str_clean(arg):\n",
    "    \"\"\"\n",
    "    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›\n",
    "    _func = np.frompyfunc(_str_clean, 1, 1)\n",
    "\n",
    "    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›\n",
    "    _list = np.array(arg, dtype=\"object\")\n",
    "\n",
    "    # çµæœã‚’å–å¾—\n",
    "    result = _func(_list)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›\n",
    "    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else \"error\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¯¾è±¡ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”¨ã®é–¢æ•°è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def click_search_with_medical_dep(driver):\n",
    "    search_buttons = driver.find_elements(By.CSS_SELECTOR, \"div[class='home-contents'] li a\")\n",
    "\n",
    "    for search_button in search_buttons:\n",
    "        if search_button.text.strip() == \"è¨ºç™‚ç§‘ç›®ã§æ¢ã™\":\n",
    "            search_button.click()\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "# -------------------------------------------------------------------------------------\n",
    "def select_city_buttons(driver):\n",
    "    select_buttons = driver.find_elements(By.CSS_SELECTOR, \"div[id='sectionIn-01'] span[class='button-label']\")\n",
    "\n",
    "    for select_button in select_buttons:\n",
    "        if select_button.text.strip() == \"ä½æ‰€ä¸€è¦§ã‹ã‚‰æŒ‡å®šã™ã‚‹\":\n",
    "            select_button.click()\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "# -------------------------------------------------------------------------------------\n",
    "def select_city(driver, cityIndex):\n",
    "    select_city_buttons(driver)\n",
    "    time.sleep(WAIT_SEC)\n",
    "\n",
    "    handle_array = driver.window_handles\n",
    "    driver.switch_to.window(handle_array[-1])\n",
    "\n",
    "    cities = driver.find_elements(By.CSS_SELECTOR, \"div[class='section-main'] a\")\n",
    "    cities[cityIndex].click()\n",
    "    time.sleep(WAIT_SEC)\n",
    "\n",
    "    select_buttons = driver.find_elements(By.CSS_SELECTOR, \"span[class='button-container']\")\n",
    "    for select_button in select_buttons:\n",
    "        if select_button.text.strip() == \"æ±ºå®š\":\n",
    "            select_button.click()\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    driver.switch_to.window(handle_array[0])\n",
    "    \n",
    "    return nCities\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def select_clinic_type(driver, clinicIndex):\n",
    "    clinic_select_boxes = driver.find_elements(By.CSS_SELECTOR, \"div[id='search-collapse-04'] div[class='col-xs-4']\")\n",
    "\n",
    "    for clinic_select_box in clinic_select_boxes:\n",
    "        if clinic_select_box.text.strip() == TARGET_CLINIC[clinicIndex]:\n",
    "            driver.execute_script('arguments[0].click();', clinic_select_box.find_elements(By.CSS_SELECTOR, \"input\")[0])\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "# -------------------------------------------------------------------------------------\n",
    "def search_button(driver):\n",
    "    buttons = driver.find_elements(By.CSS_SELECTOR, \"span[class='button-label']\")\n",
    "    for button in buttons:\n",
    "        if button.text.strip() == \"æ¤œç´¢ã™ã‚‹\":\n",
    "            button.click()\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "# -------------------------------------------------------------------------------------\n",
    "def get_page_info(driver):\n",
    "    nClinics = driver.find_elements(By.CSS_SELECTOR, \"div[class='search-list-hospital']\")\n",
    "    \n",
    "    latlon_list = []\n",
    "    latlonObjects = driver.find_elements(By.CSS_SELECTOR, \"div[class='search-list-hospital'] dd > a\")\n",
    "    \n",
    "    pattern = r\"q=([\\d.-]+),([\\d.-]+)\"   \n",
    "    for latlonObject in latlonObjects:\n",
    "        text = latlonObject.get_attribute(\"href\")\n",
    "        \n",
    "        matches = re.search(pattern, text)\n",
    "        if matches:\n",
    "            latitude = matches.group(1)\n",
    "            longitude = matches.group(2)\n",
    "        else:\n",
    "            latitude = \"na\"\n",
    "            longitude = \"na\"\n",
    "        \n",
    "        latlon_list.append([latitude, longitude])\n",
    "    \n",
    "    return len(nClinics), latlon_list\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def visit_stores(driver, storeIndex):\n",
    "    store_objects = driver.find_elements(By.CSS_SELECTOR, \"div[class='search-list-hospital-box'] table[class='table'] h3 > a\")\n",
    "    store_objects[storeIndex].click()\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def switch_window(driver):\n",
    "    original_window = driver.current_window_handle\n",
    "    handle_array = driver.window_handles\n",
    "\n",
    "    # seleniumã§æ“ä½œå¯èƒ½ãªdriverã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹\n",
    "    driver.switch_to.window(handle_array[-1])\n",
    "    \n",
    "    return original_window\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def scrape_basic_info(driver):\n",
    "    html = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    \n",
    "    updateDate = html.select(\"div[class='article-time']\")[0].text.replace(\"æœ€çµ‚å ±å‘Šæ—¥ï¼š\",\"\")\n",
    "    current_url = driver.current_url\n",
    "    timeStamp = datetime.date.today()\n",
    "    \n",
    "    basic_info = html.select(\"div[id='tabContent01']\")[0]\n",
    "    tableKeys = [str_clean(t.text.strip()).replace(\" \",\"\") for t in basic_info.select(\"table tr > th\")]\n",
    "    tableValues = [str_clean(t.text.strip()).replace(\" \",\"\") for t in basic_info.select(\"table tr > td\")]\n",
    "    \n",
    "    try:\n",
    "        store_name = tableValues[tableKeys.index(\"æ­£å¼åç§°ï¼ˆåŒ»ç™‚æ³•å±Šå‡ºæ­£å¼åç§°ï¼‰\")]\n",
    "    except:\n",
    "        store_name = \"na\"\n",
    "    try:\n",
    "        founder_type = tableValues[tableKeys.index(\"é–‹è¨­è€…ç¨®åˆ¥\")]\n",
    "    except:\n",
    "        founder_type = \"na\"\n",
    "    try:\n",
    "        founder_name = tableValues[tableKeys.index(\"é–‹è¨­è€…å\")]\n",
    "    except:\n",
    "        founder_name = \"na\"\n",
    "    try:\n",
    "        administrator_name = tableValues[tableKeys.index(\"ç®¡ç†è€…å\")]\n",
    "    except:\n",
    "        administrator_name = \"na\"\n",
    "    try:\n",
    "        store_address = tableValues[tableKeys.index(\"æ‰€åœ¨åœ°\")]\n",
    "    except:\n",
    "        store_address = \"na\"\n",
    "    \n",
    "    storeAddressOriginal = omit_postcode_tel(store_address)\n",
    "\n",
    "    try:\n",
    "        storeAddressNormalize = \"\".join(list(normalize(storeAddressOriginal).values())[0:4])\n",
    "        storeAddressNormalize_1 = _split_buildingName(storeAddressNormalize)[0]\n",
    "        storeAddressNormalize_2 = _split_buildingName(storeAddressNormalize)[1]\n",
    "    except:\n",
    "        storeAddressNormalize_1 = storeAddressNormalize_2 = \"na\"\n",
    "    \n",
    "    return [timeStamp, store_name, storeAddressOriginal, storeAddressNormalize_1, storeAddressNormalize_2, updateDate, current_url, founder_type, founder_name, administrator_name]\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def scrape_clinic_service(driver):\n",
    "    html = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    \n",
    "    service_info = html.select(\"div[id='tabContent05']\")[0]\n",
    "    clinic_tables = [t for t in service_info.select(\"table\") if t.has_attr(\"summary\")]\n",
    "    clinic_table_names = [t[\"summary\"] for t in clinic_tables]\n",
    "\n",
    "    \n",
    "    try:\n",
    "        general_service_table = clinic_tables[clinic_table_names.index(\"æ­¯ç§‘ä¸€èˆ¬é ˜åŸŸä¸€è¦§\")]\n",
    "        general_service = [str_clean(t.text.strip()).replace(\" \",\"\") for t in general_service_table.select(\"tbody th\")]\n",
    "    except:\n",
    "        general_service = \"na\"\n",
    "    \n",
    "    try:\n",
    "        oral_surgery_table = clinic_tables[clinic_table_names.index(\"æ­¯ç§‘å£è…”å¤–ç§‘é ˜åŸŸä¸€è¦§\")]\n",
    "        oral_surgery = [str_clean(t.text.strip()).replace(\" \",\"\") for t in oral_surgery_table.select(\"tbody th\")]\n",
    "    except:\n",
    "        oral_surgery = \"na\"\n",
    "\n",
    "    try:\n",
    "        kids_table = clinic_tables[clinic_table_names.index(\"å°å…æ­¯ç§‘é ˜åŸŸä¸€è¦§\")]\n",
    "        kids_service = [str_clean(t.text.strip()).replace(\" \",\"\") for t in kids_table.select(\"tbody th\")]\n",
    "    except:\n",
    "        kids_service = \"na\"\n",
    "    \n",
    "    try:\n",
    "        orthodontics_table = clinic_tables[clinic_table_names.index(\"çŸ¯æ­£æ­¯ç§‘é ˜åŸŸä¸€è¦§\")]\n",
    "        orthodontics_service = [str_clean(t.text.strip()).replace(\" \",\"\") for t in orthodontics_table.select(\"tbody th\")]\n",
    "    except:\n",
    "        orthodontics_service = \"na\"\n",
    "    \n",
    "    try:\n",
    "        facility_table = clinic_tables[clinic_table_names.index(\"æ–½è¨­çŠ¶æ³ä¸€è¦§\")]\n",
    "        facility_service = [str_clean(t.text.strip()).replace(\" \",\"\") for t in facility_table.select(\"tbody th\")]\n",
    "    except:\n",
    "        facility_service = \"na\"\n",
    "    \n",
    "    try:\n",
    "        anesthesia_table = clinic_tables[clinic_table_names.index(\"å¯¾å¿œå¯èƒ½ãªéº»é…”æ²»ç™‚ä¸€è¦§\")]\n",
    "        anesthesia_service = [str_clean(t.text.strip()).replace(\" \",\"\") for t in anesthesia_table.select(\"tbody th\")]\n",
    "    except:\n",
    "        anesthesia_service = \"na\"\n",
    "    \n",
    "    try:\n",
    "        home_therapy_table = clinic_tables[clinic_table_names.index(\"åœ¨å®…åŒ»ç™‚\")]\n",
    "        home_therapy = [str_clean(t.text.strip()).replace(\" \",\"\") for t in home_therapy_table.select(\"tbody th\")]\n",
    "    except:\n",
    "        home_therapy = \"na\"\n",
    "    \n",
    "    try:\n",
    "        collabo_service_table = clinic_tables[clinic_table_names.index(\"é€£æºã®æœ‰ç„¡\")]\n",
    "        collabo_service = [str_clean(t.text.strip()).replace(\" \",\"\") for t in collabo_service_table.select(\"tbody th\")]\n",
    "    except:\n",
    "        collabo_service = \"na\"\n",
    "    \n",
    "    return [general_service, oral_surgery, kids_service, orthodontics_service, facility_service, anesthesia_service, home_therapy, collabo_service]\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def scrape_result_info(driver):\n",
    "    html = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    \n",
    "    result_info = html.select(\"div[id='tabContent06']\")[0]\n",
    "    result_rows = result_info.select(\"table tbody tr\")\n",
    "    result_keys = [t.select(\"th\")[0].text.strip() for t in result_rows]\n",
    "    \n",
    "    try:\n",
    "        dentists = \"|\".join([t.text for t in result_rows[result_keys.index(\"æ­¯ç§‘åŒ»å¸«\")].select(\"td\")])\n",
    "    except:\n",
    "        dentists = \"na\"\n",
    "    \n",
    "    try:\n",
    "        dental_technician = \"|\".join([t.text for t in result_rows[result_keys.index(\"æ­¯ç§‘æŠ€å·¥å£«\")].select(\"td\")])\n",
    "    except:\n",
    "        dental_technician = \"na\"\n",
    "    \n",
    "    try:\n",
    "        dental_assistant = \"|\".join([t.text for t in result_rows[result_keys.index(\"æ­¯ç§‘åŠ©æ‰‹\")].select(\"td\")])\n",
    "    except:\n",
    "        dental_assistant = \"na\"\n",
    "    \n",
    "    try:\n",
    "        dental_hygienist = \"|\".join([t.text for t in result_rows[result_keys.index(\"æ­¯ç§‘è¡›ç”Ÿå£«\")].select(\"td\")])\n",
    "    except:\n",
    "        dental_hygienist = \"na\"\n",
    "    \n",
    "    try:\n",
    "        patiensts = \"|\".join([t.text for t in result_rows[result_keys.index(\"å‰å¹´åº¦ï¼‘æ—¥å¹³å‡å¤–æ¥æ‚£è€…æ•°\")].select(\"td\")])\n",
    "    except:\n",
    "        patiensts = \"na\"\n",
    "        \n",
    "        \n",
    "    return [dentists, dental_technician, dental_assistant, dental_hygienist, patiensts]\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def paging(driver, page):\n",
    "    next_page_button = driver.find_elements(By.CSS_SELECTOR, \"ul[class='hospital-pager'] li[class='next'] a\")[-1]\n",
    "    \n",
    "    if \"disabled\" in next_page_button.get_attribute(\"href\"):\n",
    "        print(\"no more pages\")\n",
    "        return False, page\n",
    "    \n",
    "    else:\n",
    "        page += 1\n",
    "        next_page_button.click()\n",
    "        return True, page\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä½æ‰€å‡¦ç†ç³»ã®é–¢æ•°è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _omit_postcode_tel(arg):\n",
    "    \"\"\"\n",
    "    éƒµä¾¿ç•ªå·ã¨é›»è©±ç•ªå·ã‚’å‰Šé™¤ã™ã‚‹å†…éƒ¨é–¢æ•°ã€‚\n",
    "    \"\"\"\n",
    "    # æ–‡å­—åˆ—ã®æ­£è¦åŒ–ã¨å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤\n",
    "    result = normalization(arg).strip()\n",
    "\n",
    "    # éƒµä¾¿ç•ªå·ã®å‰Šé™¤\n",
    "    result = re.sub(r\"ã€’.*?\\d{2,3}\\D*?\\d{3,5}\\s*\", \"\", result)\n",
    "    result = re.sub(r\"^\\d{3}\\D*?\\d{4}\\s*\", \"\", result)\n",
    "\n",
    "    # é›»è©±ç•ªå·ã®å‰Šé™¤\n",
    "    result = re.sub(r\"tel.*\\d{2,5}.*\\d{2,5}.*\\d{4}|é›»è©±.*\\d{2,5}.*\\d{2,5}.*\\d{4}\",\"\",result)\n",
    "\n",
    "    # ã€Œä½æ‰€ã€ãªã©ã®ç‰¹å®šã®å˜èªã®å‰Šé™¤\n",
    "    result = result.replace(\"ä½æ‰€\",\"\").replace(\"åœ°å›³ï½¦è¡¨ç¤º\",\"\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def omit_postcode_tel(arg):\n",
    "    ## universalize\n",
    "    _func = np.frompyfunc(_omit_postcode_tel, 1, 1)\n",
    "\n",
    "    ## list to ndarray\n",
    "    _list = np.array(arg)\n",
    "\n",
    "    ## get results\n",
    "    result = _func(_list)\n",
    "\n",
    "    ## convert data type\n",
    "    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else \"error\"\n",
    "\n",
    "    return result\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def _split_buildingName(arg):\n",
    "    \"\"\"\n",
    "    å»ºç‰©åã‚’åˆ‡ã‚Šåˆ†ã‘ã‚‹å†…éƒ¨é–¢æ•°ã€‚\n",
    "    \"\"\"\n",
    "    ## ãƒã‚¤ãƒ•ãƒ³ã®ä¸€èˆ¬åŒ–\n",
    "    address = normalization(arg)\n",
    "    hyphens = '-Ë—á…³á­¸â€â€‘â€’â€“â€”â€•âƒâ»âˆ’â–¬â”€â”â–ãƒ¼ã…¡ï¹˜ï¹£ï¼ï½°ğ„ğ†‘áš€'\n",
    "    address = re.sub(\"|\".join(hyphens), \"-\", address)\n",
    "    address = re.sub(r\"([ï½±-ï¾])(-)\",r\"\\1ï½°\", address)\n",
    "\n",
    "    ## ä¸ç›®ã€ç•ªåœ°ã€å·ãªã©ã§ä½¿ã‚ã‚Œã‚‹æ¼¢å­—ã®å®šç¾©\n",
    "    chome_poplist = [\"ï¾‰åˆ‡\",\"ç”ºç›®\",\"åœ°å‰²\",\"ä¸ç›®\",\"ä¸\",\"çµ„\",\"ç•ªç”º\",\"ç•ªåœ°\",\"ç•ªç›®\",\"ç•ª\",\"å·å®¤\",\"å·\",\"è¡—åŒº\",\"ç”»åœ°\"]\n",
    "    chome_popset = r\"|\".join(chome_poplist)\n",
    "    chome_holdlist = [\"æ¡æ±\",\"æ¡è¥¿\",\"æ¡å—\",\"æ¡åŒ—\",\"æ¡é€š\",\"æ¡\",\"æ±\",\"è¥¿\",\"å—\",\"åŒ—\"]\n",
    "    chome_holdset = r\"|\".join(chome_holdlist)\n",
    "    chome_alllist = chome_popset + chome_holdset\n",
    "    chome_allset = r\"|\".join(chome_alllist)\n",
    "\n",
    "    ## separate address\n",
    "    result = re.findall(re.compile(f\"(.*\\d\\[{chome_allset}\\]*)|(\\D+\\[-\\d\\]+)|(.*)\"), address)\n",
    "\n",
    "    ## convert kanji into hyphen\n",
    "    result = [[re.sub(f\"(\\d+)({chome_popset})\", r\"\\1-\", \"\".join(t)) for t in tl] for tl in result]\n",
    "\n",
    "    ## concat all\n",
    "    result = [\"\".join(t) for t in result]\n",
    "    result = \"\".join(result)\n",
    "\n",
    "    ## special case handling (1ï¾‰3 1åŒº1)\n",
    "    result = re.sub(r\"([^ï½±ï½°ï¾])(ï¾‰|ï½°)(\\d)\", r\"\\1-\\3\", result)\n",
    "    result = re.sub(r\"(\\d)(åŒº)(\\d)\", r\"\\1-\\3\", result)\n",
    "    result = re.sub(\"--\", \"-\", result)\n",
    "\n",
    "    ## separate into [japanese] + [number + hyphen] chunks\n",
    "    result = re.findall(re.compile(f\"(\\D+[-\\d]+[{chome_holdset}]*[-\\d]+)|(\\D+[-\\d]+)|(.*)\"), result)\n",
    "    result = [t for t in [\"\".join(tl) for tl in result] if t != \"\"]\n",
    "\n",
    "    ## merge [number + hyphen] chunks\n",
    "    try:\n",
    "        result = [result[0]] + [\"\".join(result[1:])]\n",
    "    except:\n",
    "        result = result\n",
    "\n",
    "    # 2åˆ—ç›®ãŒå˜ç‹¬ã€Œf, éšã€ã®ã¨ãã€1åˆ—ç›®ã®æœ«å°¾æ•°ã‚’2åˆ—ç›®ã¸ç§»å‹•\n",
    "    if re.fullmatch(r\"f|éš\", result[1]):\n",
    "        result[1] = \"\".join(re.compile(r\"\\d+$\").findall(result[0])) + result[1]\n",
    "        result[0] = re.sub(r\"\\d+$\", \"\", result[0])\n",
    "\n",
    "    # 2åˆ—ç›®ã§ã€éšæ•°ãŒç•ªåœ°ã¨çµåˆã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã¨ãã€éšæ•°ã‚’1æ¡ã¨ã¿ãªã—ã€æ®‹ã‚Šã®æ•°å­—ã‚’ç•ªåœ°ã¨ã—ã¦1åˆ—ç›®ã¸ç§»å‹•\n",
    "    if (re.fullmatch(r\"\\D+\", result[0]) or re.search(r\"-$\", result[0])) and re.match(r\"(\\d*)(\\d)(f|éš)(\\d*)\", result[1]):\n",
    "        result[1] = re.sub(r\"(\\d*)(\\d)(f|éš)(\\d*)\", r\"\\1,\\2\\3\\4\", result[1])\n",
    "        result[0] = result[0] + result[1][:result[1].find(\",\")]\n",
    "        result[1] = result[1][result[1].find(\",\")+1:]\n",
    "\n",
    "    # æœ«å°¾ã®ãƒã‚¤ãƒ•ãƒ³ã‚’å‰Šé™¤\n",
    "    result[0] = re.sub(r\"-+$\", \"\", result[0])\n",
    "\n",
    "    return result\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def split_buildingName(arg):\n",
    "    ## universalize\n",
    "    _func = np.frompyfunc(_split_buildingName, 1, 1)\n",
    "\n",
    "    ## list to ndarray\n",
    "    _list = np.array(arg)\n",
    "\n",
    "    ## get results\n",
    "    result = _func(_list)\n",
    "\n",
    "    ## convert data type\n",
    "    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else \"error\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç³»ã®é–¢æ•°è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_columns(FLAG, args):\n",
    "    if FLAG:\n",
    "        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è¨­å®š\n",
    "        csvlist = [[\n",
    "        \"timeStamp\",\n",
    "        \"storeName\",\n",
    "        \"address_original\", \n",
    "        \"address_normalize[0]\",\n",
    "        \"address_normalize[1]\"\n",
    "        ] + args]\n",
    "    else:\n",
    "        # ç©ºã®ãƒªã‚¹ãƒˆã‚’è¨­å®š\n",
    "        csvlist = []\n",
    "    return csvlist\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "def write_to_csv(EXPORT_PATH, SOURCE_NAME, dt_now, page, csvlist):\n",
    "    max_attemts = 5  # æœ€å¤§è©¦è¡Œå›æ•°\n",
    "    delay_between_attempts = 60  # è©¦è¡Œé–“ã®é…å»¶ï¼ˆç§’ï¼‰\n",
    "\n",
    "    # æ–‡å­—åˆ—ã®æ­£è¦åŒ–\n",
    "    csvlist = normalization(csvlist)\n",
    "\n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿è©¦è¡Œ\n",
    "    for i in range(max_attemts):\n",
    "        try:\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãã€CSVã«æ›¸ãè¾¼ã‚€\n",
    "            with open(EXPORT_PATH + \"/\" + SOURCE_NAME + \"_\"  + str(dt_now.year) + \"-\" + str(dt_now.month) + \"-\" + str(dt_now.day) + \"-\" + str(i) + \".csv\", \"a\", newline=\"\", encoding=\"CP932\", errors=\"replace\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                print(f\"now exported page:{page}\", f\"extracted {len(csvlist)} stores\")\n",
    "                writer.writerows(csvlist)\n",
    "                break\n",
    "        except OSError as e:\n",
    "            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®å‡¦ç†\n",
    "            if i < max_attemts - 1:\n",
    "                time.sleep(delay_between_attempts)\n",
    "                print(f\"OSError: {e}. Retrying...\")\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### config\n",
    "startCityIndex = 0 # min 0\n",
    "startClinicIndex = 0 # min 0\n",
    "startPage = 1 # min 1\n",
    "startStoreIndex = 0 # min 0\n",
    "nCities = 1 # åƒä»£ç”°åŒºã§æ¤œç´¢ã—ã¦ã‚‚æ±äº¬éƒ½å…¨ä½“ãŒæ¤œç´¢ã•ã‚Œã‚‹\n",
    "TARGET_CLINIC = [\"æ­¯ç§‘\", \"çŸ¯æ­£æ­¯ç§‘\", \"å°å…æ­¯ç§‘\", \"æ­¯ç§‘å£è…”å¤–ç§‘\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### open with selenium\n",
    "driver = start_driver()\n",
    "driver.maximize_window()\n",
    "driver.get(START_URL)\n",
    "time.sleep(WAIT_SEC + np.random.rand()*WAIT_SEC)  \n",
    "\n",
    "### set csv format\n",
    "args = [\"æœ€çµ‚æ›´æ–°æ—¥\", \"URL\", \"é–‹è¨­è€…ç¨®åˆ¥\", \"é–‹è¨­è€…å\", \"ç®¡ç†è€…å\", \"æ­¯ç§‘ä¸€èˆ¬é ˜åŸŸä¸€è¦§\", \"æ­¯ç§‘å£è…”å¤–ç§‘é ˜åŸŸä¸€è¦§\", \"å°å…æ­¯ç§‘é ˜åŸŸä¸€è¦§\", \"çŸ¯æ­£æ­¯ç§‘é ˜åŸŸä¸€è¦§\", \"æ–½è¨­çŠ¶æ³ä¸€è¦§\", \"å¯¾å¿œå¯èƒ½ãªéº»é…”æ²»ç™‚ä¸€è¦§\", \"åœ¨å®…åŒ»ç™‚\", \"é€£æºã®æœ‰ç„¡\", \"æ­¯ç§‘åŒ»å¸«ï¼ˆç·æ•°|å¸¸å‹¤|éå¸¸å‹¤ï¼‰\", \"æ­¯ç§‘æŠ€å·¥å£«ï¼ˆç·æ•°|å¸¸å‹¤|éå¸¸å‹¤ï¼‰\", \"æ­¯ç§‘åŠ©æ‰‹ï¼ˆç·æ•°|å¸¸å‹¤|éå¸¸å‹¤ï¼‰\", \"æ­¯ç§‘è¡›ç”Ÿå£«ï¼ˆç·æ•°|å¸¸å‹¤|éå¸¸å‹¤ï¼‰\", \"å‰å¹´åº¦ï¼‘æ—¥å¹³å‡å¤–æ¥æ‚£è€…æ•°\", \"ç·¯åº¦\", \"çµŒåº¦\", \"page\"]\n",
    "FLAG = startCityIndex == 0 and startClinicIndex == 0 and startPage == 1 and startStoreIndex == 0\n",
    "csvlist_header = set_columns(FLAG, args)\n",
    "csvlist = []\n",
    "\n",
    "click_search_with_medical_dep(driver)\n",
    "time.sleep(WAIT_SEC + np.random.rand()*WAIT_SEC)  \n",
    "select_city_buttons(driver)\n",
    "\n",
    "for cityIndex in range(startCityIndex, nCities):\n",
    "    nCities = select_city(driver, cityIndex)\n",
    "\n",
    "    start_clinic_index = startClinicIndex if cityIndex == startCityIndex else 0\n",
    "    for clinicIndex in range(start_clinic_index, len(TARGET_CLINIC)):\n",
    "        select_clinic_type(driver, clinicIndex)\n",
    "        search_button(driver)\n",
    "        \n",
    "        ## initial paging\n",
    "        for i in range(startPage - 1):\n",
    "            flag, page = paging(driver, page)\n",
    "            time.sleep(WAIT_SEC + np.random.rand()*WAIT_SEC)\n",
    "          \n",
    "        while True:\n",
    "            ## get list info\n",
    "            nClinics, latlon_list = get_page_info(driver) \n",
    "            start_store_index = startStoreIndex if page == startPage and clinicIndex == startClinicIndex and cityIndex == startCityIndex else 0\n",
    "            \n",
    "            for storeIndex in range(start_store_index, nClinics):                \n",
    "                # scrape info\n",
    "                time.sleep(WAIT_SEC + np.random.rand()*WAIT_SEC)  \n",
    "                visit_stores(driver, storeIndex)\n",
    "                original_window = switch_window(driver)\n",
    "                \n",
    "                basic_info = scrape_basic_info(driver)\n",
    "                service_info = scrape_clinic_service(driver)\n",
    "                result_info = scrape_result_info(driver)\n",
    "                _row = basic_info + service_info + result_info + latlon_list[storeIndex] + [page]\n",
    "\n",
    "                #Close the tab or window\n",
    "                driver.close()\n",
    "                driver.switch_to.window(original_window)\n",
    "\n",
    "                ## store data\n",
    "                csvlist.append(_row)\n",
    "\n",
    "                ## record\n",
    "                if FLAG:\n",
    "                    FLAG = False\n",
    "                    write_to_csv(EXPORT_PATH, SOURCE_NAME, dt_now, page, csvlist_header)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                write_to_csv(EXPORT_PATH, SOURCE_NAME, dt_now, page, csvlist)\n",
    "                csvlist = []\n",
    "\n",
    "                latestCityIndex = cityIndex # min 0\n",
    "                latestClinicIndex = clinicIndex # min 0\n",
    "                latestPage = page # min 1\n",
    "                latestStore = storeIndex # min 0\n",
    "\n",
    "            #paging\n",
    "            flag, page = paging(driver, page)\n",
    "\n",
    "            if flag:\n",
    "                pass\n",
    "            else:\n",
    "                print(\"going to the next category...\")\n",
    "                break\n",
    "\n",
    "print(\"done\")\n",
    "driver.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
