#!/usr/bin/env python
# coding: utf-8

# #### ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€URLãªã©å„ç¨®è¨­å®š

# In[1]:


### import necessary library
from bs4 import BeautifulSoup
import csv
import datetime
import getpass
import jaconv
from normalize_japanese_addresses import normalize
import numpy as np
import pandas as pd
import random
import re
import requests
import random
import selenium
from selenium import webdriver
import sys
import time

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

from urllib.parse import urljoin


# In[2]:


### version
print("Selenium: ", selenium.__version__)
print("Python: ",sys.version)


# In[3]:


### individual configure
SOURCE_NAME = "medicalInfoService" 
BASE_URL = "https://www.himawari.metro.tokyo.jp/qq13/qqport/tomintop/"
START_URL = "https://www.himawari.metro.tokyo.jp/qq13/qqport/tomintop/"
WAIT_SEC = 5
maxTry = 5
dt_now = datetime.datetime.now()
page = 1
num = 0
EXPORT_PATH = r"shops"


# #### ã‚¦ã‚§ãƒ–ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ç³»ã®é–¢æ•°è¨­å®š

# In[4]:


def start_driver():
    # Seleniumç”¨ã®ã‚¦ã‚§ãƒ–ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–ã—ã€ã•ã¾ã–ã¾ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å®‰å®šã—ãŸæœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ã€‚
    # Seleniumç”¨ã®Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¨­å®šã€‚
    options = webdriver.ChromeOptions()
    options.add_argument('--disable-extensions')  # ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãŸã‚ã«ãƒ–ãƒ©ã‚¦ã‚¶æ‹¡å¼µã‚’ç„¡åŠ¹ã«ã™ã‚‹ã€‚
    options.add_argument('--start-maximized')  # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’æœ€å¤§åŒ–ã—ãŸã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§é–‹å§‹ã€‚å‚è€ƒ: https://stackoverflow.com/a/26283818/1689770
    options.add_argument('--no-sandbox')  # äº’æ›æ€§å‘ä¸Šã®ãŸã‚ã«ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ã‚’ç„¡åŠ¹ã«ã™ã‚‹ã€‚å‚è€ƒ: https://stackoverflow.com/a/50725918/1689770
    options.add_argument('--disable-dev-shm-usage')  # ã‚ˆã‚Šå®‰å®šã—ãŸå‹•ä½œã®ãŸã‚ã«ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã€‚å‚è€ƒ: https://stackoverflow.com/a/50725918/1689770

    # ä¸»å‡¦ç†
    try:
        driver_path = ChromeDriverManager().install()
        service = Service(executable_path=driver_path)
        driver = webdriver.Chrome(service=service, options=options)

    except ValueError:
        # æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®Chromeãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’å–å¾—ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚
        url = r'https://googlechromelabs.github.io/chrome-for-testing/last-known-good-versions-with-downloads.json'
        response = requests.get(url)
        data_dict = response.json()
        latest_version = data_dict["channels"]["Stable"]["version"]

        driver_path = ChromeDriverManager(version=latest_version).install()
        service = Service(executable_path=driver_path)
        driver = webdriver.Chrome(service=service, options=options)

    except PermissionError:  # æš«å®šå‡¦ç† å‚è€ƒ: https://note.com/yuu________/n/n14d97c155e5e
        try:
            driver = webdriver.Chrome(service=Service(f'C:\\Users\\{USERNAME}\\.wdm\\drivers\\chromedriver\\win64\\116.0.5845.97\\chromedriver.exe'), options=options)
        except:
            driver = webdriver.Chrome(service=Service(f'C:\\Users\\{USERNAME}\\.wdm\\drivers\\chromedriver\\win64\\116.0.5845.96\\chromedriver.exe'), options=options)

    # ãƒ–ãƒ©ã‚¦ã‚¶ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æœ€å¤§åŒ–ã€‚
    driver.maximize_window()
    # ã‚¦ã‚§ãƒ–ãƒ‰ãƒ©ã‚¤ãƒã®å¾…æ©Ÿæ™‚é–“ã‚’è¨­å®šã€‚
    wait = WebDriverWait(driver, WAIT_SEC)

    return driver


# #### æ–‡å­—åˆ—æ“ä½œç³»ã®é–¢æ•°è¨­å®š

# In[5]:


# -------------------------------------------------------------------------------------
def _normalization(arg):
    """
    æ–‡å­—åˆ—ã®æ­£è¦åŒ–ã‚’è¡Œã†å†…éƒ¨é–¢æ•°ã€‚
    ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«ã€å…¨è§’ã‚’åŠè§’ã«ã€å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã€ä¸å¯è¦–æ–‡å­—ã‚‚å‰Šé™¤ã™ã‚‹ã€‚
    """

    try:
        # ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›
        try:
            result = jaconv.hira2kata(arg)
        except AttributeError:
            result = arg

        # å…¨è§’ã‚’åŠè§’ã«å¤‰æ›
        try:
            result = jaconv.z2h(result, digit=True, ascii=True)
        except AttributeError:
            result = result

        # å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›
        try:
            result = result.lower()
        except AttributeError:
            result = result

        # ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤
        try:
            result = _str_clean(result)
        except TypeError:
            result = result

    except:
        result = arg

    return result

# -------------------------------------------------------------------------------------
def normalization(arg):
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_normalization, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _str_clean(arg):
    """
    æ–‡å­—åˆ—ã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """

    try:
        result = arg.strip()
    except:
        result = arg

    try:
        result = re.sub(r"\r|\n|\r\n|\u3000|\t|ã€€| |,", " ", result)
    except TypeError:
        result = result

    return result

# -------------------------------------------------------------------------------------
def str_clean(arg):
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_str_clean, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result


# #### å¯¾è±¡ã‚µã‚¤ãƒˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”¨ã®é–¢æ•°è¨­å®š

# In[6]:


def click_search_with_medical_dep(driver):
    search_buttons = driver.find_elements(By.CSS_SELECTOR, "div[class='home-contents'] li a")

    for search_button in search_buttons:
        if search_button.text.strip() == "è¨ºç™‚ç§‘ç›®ã§æ¢ã™":
            search_button.click()
            break
        else:
            pass
        
# -------------------------------------------------------------------------------------
def select_city_buttons(driver):
    select_buttons = driver.find_elements(By.CSS_SELECTOR, "div[id='sectionIn-01'] span[class='button-label']")

    for select_button in select_buttons:
        if select_button.text.strip() == "ä½æ‰€ä¸€è¦§ã‹ã‚‰æŒ‡å®šã™ã‚‹":
            select_button.click()
            break
        else:
            pass
        
# -------------------------------------------------------------------------------------
def select_city(driver, cityIndex):
    select_city_buttons(driver)
    time.sleep(WAIT_SEC)

    handle_array = driver.window_handles
    driver.switch_to.window(handle_array[-1])

    cities = driver.find_elements(By.CSS_SELECTOR, "div[class='section-main'] a")
    cities[cityIndex].click()
    time.sleep(WAIT_SEC)

    select_buttons = driver.find_elements(By.CSS_SELECTOR, "span[class='button-container']")
    for select_button in select_buttons:
        if select_button.text.strip() == "æ±ºå®š":
            select_button.click()
            break
        else:
            pass
    
    driver.switch_to.window(handle_array[0])
    
    return nCities

# -------------------------------------------------------------------------------------
def select_clinic_type(driver, clinicIndex):
    clinic_select_boxes = driver.find_elements(By.CSS_SELECTOR, "div[id='search-collapse-04'] div[class='col-xs-4']")

    for clinic_select_box in clinic_select_boxes:
        if clinic_select_box.text.strip() == TARGET_CLINIC[clinicIndex]:
            driver.execute_script('arguments[0].click();', clinic_select_box.find_elements(By.CSS_SELECTOR, "input")[0])
            break
        else:
            pass
        
# -------------------------------------------------------------------------------------
def search_button(driver):
    buttons = driver.find_elements(By.CSS_SELECTOR, "span[class='button-label']")
    for button in buttons:
        if button.text.strip() == "æ¤œç´¢ã™ã‚‹":
            button.click()
            break
        else:
            pass
        
# -------------------------------------------------------------------------------------
def get_page_info(driver):
    nClinics = driver.find_elements(By.CSS_SELECTOR, "div[class='search-list-hospital']")
    
    latlon_list = []
    latlonObjects = driver.find_elements(By.CSS_SELECTOR, "div[class='search-list-hospital'] dd > a")
    
    pattern = r"q=([\d.-]+),([\d.-]+)"   
    for latlonObject in latlonObjects:
        text = latlonObject.get_attribute("href")
        
        matches = re.search(pattern, text)
        if matches:
            latitude = matches.group(1)
            longitude = matches.group(2)
        else:
            latitude = "na"
            longitude = "na"
        
        latlon_list.append([latitude, longitude])
    
    return len(nClinics), latlon_list

# -------------------------------------------------------------------------------------
def visit_stores(driver, storeIndex):
    store_objects = driver.find_elements(By.CSS_SELECTOR, "div[class='search-list-hospital-box'] table[class='table'] h3 > a")
    store_objects[storeIndex].click()

# -------------------------------------------------------------------------------------
def switch_window(driver):
    original_window = driver.current_window_handle
    handle_array = driver.window_handles

    # seleniumã§æ“ä½œå¯èƒ½ãªdriverã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹
    driver.switch_to.window(handle_array[-1])
    
    return original_window

# -------------------------------------------------------------------------------------
def scrape_basic_info(driver):
    html = BeautifulSoup(driver.page_source, "lxml")
    
    updateDate = html.select("div[class='article-time']")[0].text.replace("æœ€çµ‚å ±å‘Šæ—¥ï¼š","")
    current_url = driver.current_url
    timeStamp = datetime.date.today()
    
    basic_info = html.select("div[id='tabContent01']")[0]
    tableKeys = [str_clean(t.text.strip()).replace(" ","") for t in basic_info.select("table tr > th")]
    tableValues = [str_clean(t.text.strip()).replace(" ","") for t in basic_info.select("table tr > td")]
    
    try:
        store_name = tableValues[tableKeys.index("æ­£å¼åç§°ï¼ˆåŒ»ç™‚æ³•å±Šå‡ºæ­£å¼åç§°ï¼‰")]
    except:
        store_name = "na"
    try:
        founder_type = tableValues[tableKeys.index("é–‹è¨­è€…ç¨®åˆ¥")]
    except:
        founder_type = "na"
    try:
        founder_name = tableValues[tableKeys.index("é–‹è¨­è€…å")]
    except:
        founder_name = "na"
    try:
        administrator_name = tableValues[tableKeys.index("ç®¡ç†è€…å")]
    except:
        administrator_name = "na"
    try:
        store_address = tableValues[tableKeys.index("æ‰€åœ¨åœ°")]
    except:
        store_address = "na"
    
    storeAddressOriginal = omit_postcode_tel(store_address)

    try:
        storeAddressNormalize = "".join(list(normalize(storeAddressOriginal).values())[0:4])
        storeAddressNormalize_1 = _split_buildingName(storeAddressNormalize)[0]
        storeAddressNormalize_2 = _split_buildingName(storeAddressNormalize)[1]
    except:
        storeAddressNormalize_1 = storeAddressNormalize_2 = "na"
    
    return [timeStamp, store_name, storeAddressOriginal, storeAddressNormalize_1, storeAddressNormalize_2, updateDate, current_url, founder_type, founder_name, administrator_name]

# -------------------------------------------------------------------------------------
def scrape_clinic_service(driver):
    html = BeautifulSoup(driver.page_source, "lxml")
    
    service_info = html.select("div[id='tabContent05']")[0]
    clinic_tables = [t for t in service_info.select("table") if t.has_attr("summary")]
    clinic_table_names = [t["summary"] for t in clinic_tables]

    
    try:
        general_service_table = clinic_tables[clinic_table_names.index("æ­¯ç§‘ä¸€èˆ¬é ˜åŸŸä¸€è¦§")]
        general_service = [str_clean(t.text.strip()).replace(" ","") for t in general_service_table.select("tbody th")]
    except:
        general_service = "na"
    
    try:
        oral_surgery_table = clinic_tables[clinic_table_names.index("æ­¯ç§‘å£è…”å¤–ç§‘é ˜åŸŸä¸€è¦§")]
        oral_surgery = [str_clean(t.text.strip()).replace(" ","") for t in oral_surgery_table.select("tbody th")]
    except:
        oral_surgery = "na"

    try:
        kids_table = clinic_tables[clinic_table_names.index("å°å…æ­¯ç§‘é ˜åŸŸä¸€è¦§")]
        kids_service = [str_clean(t.text.strip()).replace(" ","") for t in kids_table.select("tbody th")]
    except:
        kids_service = "na"
    
    try:
        orthodontics_table = clinic_tables[clinic_table_names.index("çŸ¯æ­£æ­¯ç§‘é ˜åŸŸä¸€è¦§")]
        orthodontics_service = [str_clean(t.text.strip()).replace(" ","") for t in orthodontics_table.select("tbody th")]
    except:
        orthodontics_service = "na"
    
    try:
        facility_table = clinic_tables[clinic_table_names.index("æ–½è¨­çŠ¶æ³ä¸€è¦§")]
        facility_service = [str_clean(t.text.strip()).replace(" ","") for t in facility_table.select("tbody th")]
    except:
        facility_service = "na"
    
    try:
        anesthesia_table = clinic_tables[clinic_table_names.index("å¯¾å¿œå¯èƒ½ãªéº»é…”æ²»ç™‚ä¸€è¦§")]
        anesthesia_service = [str_clean(t.text.strip()).replace(" ","") for t in anesthesia_table.select("tbody th")]
    except:
        anesthesia_service = "na"
    
    try:
        home_therapy_table = clinic_tables[clinic_table_names.index("åœ¨å®…åŒ»ç™‚")]
        home_therapy = [str_clean(t.text.strip()).replace(" ","") for t in home_therapy_table.select("tbody th")]
    except:
        home_therapy = "na"
    
    try:
        collabo_service_table = clinic_tables[clinic_table_names.index("é€£æºã®æœ‰ç„¡")]
        collabo_service = [str_clean(t.text.strip()).replace(" ","") for t in collabo_service_table.select("tbody th")]
    except:
        collabo_service = "na"
    
    return [general_service, oral_surgery, kids_service, orthodontics_service, facility_service, anesthesia_service, home_therapy, collabo_service]

# -------------------------------------------------------------------------------------
def scrape_result_info(driver):
    html = BeautifulSoup(driver.page_source, "lxml")
    
    result_info = html.select("div[id='tabContent06']")[0]
    result_rows = result_info.select("table tbody tr")
    result_keys = [t.select("th")[0].text.strip() for t in result_rows]
    
    try:
        dentists = "|".join([t.text for t in result_rows[result_keys.index("æ­¯ç§‘åŒ»å¸«")].select("td")])
    except:
        dentists = "na"
    
    try:
        dental_technician = "|".join([t.text for t in result_rows[result_keys.index("æ­¯ç§‘æŠ€å·¥å£«")].select("td")])
    except:
        dental_technician = "na"
    
    try:
        dental_assistant = "|".join([t.text for t in result_rows[result_keys.index("æ­¯ç§‘åŠ©æ‰‹")].select("td")])
    except:
        dental_assistant = "na"
    
    try:
        dental_hygienist = "|".join([t.text for t in result_rows[result_keys.index("æ­¯ç§‘è¡›ç”Ÿå£«")].select("td")])
    except:
        dental_hygienist = "na"
    
    try:
        patiensts = "|".join([t.text for t in result_rows[result_keys.index("å‰å¹´åº¦ï¼‘æ—¥å¹³å‡å¤–æ¥æ‚£è€…æ•°")].select("td")])
    except:
        patiensts = "na"
        
        
    return [dentists, dental_technician, dental_assistant, dental_hygienist, patiensts]

# -------------------------------------------------------------------------------------
def paging(driver, page):
    next_page_button = driver.find_elements(By.CSS_SELECTOR, "ul[class='hospital-pager'] li[class='next'] a")[-1]
    
    if "disabled" in next_page_button.get_attribute("href"):
        print("no more pages")
        return False, page
    
    else:
        page += 1
        next_page_button.click()
        return True, page
    


# #### ä½æ‰€å‡¦ç†ç³»ã®é–¢æ•°è¨­å®š

# In[7]:


def _omit_postcode_tel(arg):
    """
    éƒµä¾¿ç•ªå·ã¨é›»è©±ç•ªå·ã‚’å‰Šé™¤ã™ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """
    # æ–‡å­—åˆ—ã®æ­£è¦åŒ–ã¨å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
    result = normalization(arg).strip()

    # éƒµä¾¿ç•ªå·ã®å‰Šé™¤
    result = re.sub(r"ã€’.*?\d{2,3}\D*?\d{3,5}\s*", "", result)
    result = re.sub(r"^\d{3}\D*?\d{4}\s*", "", result)

    # é›»è©±ç•ªå·ã®å‰Šé™¤
    result = re.sub(r"tel.*\d{2,5}.*\d{2,5}.*\d{4}|é›»è©±.*\d{2,5}.*\d{2,5}.*\d{4}","",result)

    # ã€Œä½æ‰€ã€ãªã©ã®ç‰¹å®šã®å˜èªã®å‰Šé™¤
    result = result.replace("ä½æ‰€","").replace("åœ°å›³ï½¦è¡¨ç¤º","")

    return result

# -------------------------------------------------------------------------------------
def omit_postcode_tel(arg):
    ## universalize
    _func = np.frompyfunc(_omit_postcode_tel, 1, 1)

    ## list to ndarray
    _list = np.array(arg)

    ## get results
    result = _func(_list)

    ## convert data type
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _split_buildingName(arg):
    """
    å»ºç‰©åã‚’åˆ‡ã‚Šåˆ†ã‘ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """
    ## ãƒã‚¤ãƒ•ãƒ³ã®ä¸€èˆ¬åŒ–
    address = normalization(arg)
    hyphens = '-Ë—á…³á­¸â€â€‘â€’â€“â€”â€•âƒâ»âˆ’â–¬â”€â”â–ãƒ¼ã…¡ï¹˜ï¹£ï¼ï½°ğ„ğ†‘áš€'
    address = re.sub("|".join(hyphens), "-", address)
    address = re.sub(r"([ï½±-ï¾])(-)",r"\1ï½°", address)

    ## ä¸ç›®ã€ç•ªåœ°ã€å·ãªã©ã§ä½¿ã‚ã‚Œã‚‹æ¼¢å­—ã®å®šç¾©
    chome_poplist = ["ï¾‰åˆ‡","ç”ºç›®","åœ°å‰²","ä¸ç›®","ä¸","çµ„","ç•ªç”º","ç•ªåœ°","ç•ªç›®","ç•ª","å·å®¤","å·","è¡—åŒº","ç”»åœ°"]
    chome_popset = r"|".join(chome_poplist)
    chome_holdlist = ["æ¡æ±","æ¡è¥¿","æ¡å—","æ¡åŒ—","æ¡é€š","æ¡","æ±","è¥¿","å—","åŒ—"]
    chome_holdset = r"|".join(chome_holdlist)
    chome_alllist = chome_popset + chome_holdset
    chome_allset = r"|".join(chome_alllist)

    ## separate address
    result = re.findall(re.compile(f"(.*\d\[{chome_allset}\]*)|(\D+\[-\d\]+)|(.*)"), address)

    ## convert kanji into hyphen
    result = [[re.sub(f"(\d+)({chome_popset})", r"\1-", "".join(t)) for t in tl] for tl in result]

    ## concat all
    result = ["".join(t) for t in result]
    result = "".join(result)

    ## special case handling (1ï¾‰3 1åŒº1)
    result = re.sub(r"([^ï½±ï½°ï¾])(ï¾‰|ï½°)(\d)", r"\1-\3", result)
    result = re.sub(r"(\d)(åŒº)(\d)", r"\1-\3", result)
    result = re.sub("--", "-", result)

    ## separate into [japanese] + [number + hyphen] chunks
    result = re.findall(re.compile(f"(\D+[-\d]+[{chome_holdset}]*[-\d]+)|(\D+[-\d]+)|(.*)"), result)
    result = [t for t in ["".join(tl) for tl in result] if t != ""]

    ## merge [number + hyphen] chunks
    try:
        result = [result[0]] + ["".join(result[1:])]
    except:
        result = result

    # 2åˆ—ç›®ãŒå˜ç‹¬ã€Œf, éšã€ã®ã¨ãã€1åˆ—ç›®ã®æœ«å°¾æ•°ã‚’2åˆ—ç›®ã¸ç§»å‹•
    if re.fullmatch(r"f|éš", result[1]):
        result[1] = "".join(re.compile(r"\d+$").findall(result[0])) + result[1]
        result[0] = re.sub(r"\d+$", "", result[0])

    # 2åˆ—ç›®ã§ã€éšæ•°ãŒç•ªåœ°ã¨çµåˆã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã¨ãã€éšæ•°ã‚’1æ¡ã¨ã¿ãªã—ã€æ®‹ã‚Šã®æ•°å­—ã‚’ç•ªåœ°ã¨ã—ã¦1åˆ—ç›®ã¸ç§»å‹•
    if (re.fullmatch(r"\D+", result[0]) or re.search(r"-$", result[0])) and re.match(r"(\d*)(\d)(f|éš)(\d*)", result[1]):
        result[1] = re.sub(r"(\d*)(\d)(f|éš)(\d*)", r"\1,\2\3\4", result[1])
        result[0] = result[0] + result[1][:result[1].find(",")]
        result[1] = result[1][result[1].find(",")+1:]

    # æœ«å°¾ã®ãƒã‚¤ãƒ•ãƒ³ã‚’å‰Šé™¤
    result[0] = re.sub(r"-+$", "", result[0])

    return result

# -------------------------------------------------------------------------------------
def split_buildingName(arg):
    ## universalize
    _func = np.frompyfunc(_split_buildingName, 1, 1)

    ## list to ndarray
    _list = np.array(arg)

    ## get results
    result = _func(_list)

    ## convert data type
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result


# #### ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç³»ã®é–¢æ•°è¨­å®š

# In[8]:


def set_columns(FLAG, args):
    if FLAG:
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è¨­å®š
        csvlist = [[
        "timeStamp",
        "storeName",
        "address_original", 
        "address_normalize[0]",
        "address_normalize[1]"
        ] + args]
    else:
        # ç©ºã®ãƒªã‚¹ãƒˆã‚’è¨­å®š
        csvlist = []
    return csvlist


# -------------------------------------------------------------------------------------
def write_to_csv(EXPORT_PATH, SOURCE_NAME, dt_now, page, csvlist):
    max_attemts = 5  # æœ€å¤§è©¦è¡Œå›æ•°
    delay_between_attempts = 60  # è©¦è¡Œé–“ã®é…å»¶ï¼ˆç§’ï¼‰

    # æ–‡å­—åˆ—ã®æ­£è¦åŒ–
    csvlist = normalization(csvlist)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿è©¦è¡Œ
    for i in range(max_attemts):
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ãã€CSVã«æ›¸ãè¾¼ã‚€
            with open(EXPORT_PATH + "/" + SOURCE_NAME + "_"  + str(dt_now.year) + "-" + str(dt_now.month) + "-" + str(dt_now.day) + "-" + str(i) + ".csv", "a", newline="", encoding="CP932", errors="replace") as f:
                writer = csv.writer(f)
                print(f"now exported page:{page}", f"extracted {len(csvlist)} stores")
                writer.writerows(csvlist)
                break
        except OSError as e:
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã®å‡¦ç†
            if i < max_attemts - 1:
                time.sleep(delay_between_attempts)
                print(f"OSError: {e}. Retrying...")
                continue
            else:
                raise
                


# #### ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ¡ã‚¤ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰

# In[9]:


### config
startCityIndex = 0 # min 0
startClinicIndex = 0 # min 0
startPage = 1 # min 1
startStoreIndex = 0 # min 0
nCities = 1 # åƒä»£ç”°åŒºã§æ¤œç´¢ã—ã¦ã‚‚æ±äº¬éƒ½å…¨ä½“ãŒæ¤œç´¢ã•ã‚Œã‚‹
TARGET_CLINIC = ["æ­¯ç§‘", "çŸ¯æ­£æ­¯ç§‘", "å°å…æ­¯ç§‘", "æ­¯ç§‘å£è…”å¤–ç§‘"]


# In[ ]:


### open with selenium
driver = start_driver()
driver.maximize_window()
driver.get(START_URL)
time.sleep(WAIT_SEC + np.random.rand()*WAIT_SEC)  

### set csv format
args = ["æœ€çµ‚æ›´æ–°æ—¥", "URL", "é–‹è¨­è€…ç¨®åˆ¥", "é–‹è¨­è€…å", "ç®¡ç†è€…å", "æ­¯ç§‘ä¸€èˆ¬é ˜åŸŸä¸€è¦§", "æ­¯ç§‘å£è…”å¤–ç§‘é ˜åŸŸä¸€è¦§", "å°å…æ­¯ç§‘é ˜åŸŸä¸€è¦§", "çŸ¯æ­£æ­¯ç§‘é ˜åŸŸä¸€è¦§", "æ–½è¨­çŠ¶æ³ä¸€è¦§", "å¯¾å¿œå¯èƒ½ãªéº»é…”æ²»ç™‚ä¸€è¦§", "åœ¨å®…åŒ»ç™‚", "é€£æºã®æœ‰ç„¡", "æ­¯ç§‘åŒ»å¸«ï¼ˆç·æ•°|å¸¸å‹¤|éå¸¸å‹¤ï¼‰", "æ­¯ç§‘æŠ€å·¥å£«ï¼ˆç·æ•°|å¸¸å‹¤|éå¸¸å‹¤ï¼‰", "æ­¯ç§‘åŠ©æ‰‹ï¼ˆç·æ•°|å¸¸å‹¤|éå¸¸å‹¤ï¼‰", "æ­¯ç§‘è¡›ç”Ÿå£«ï¼ˆç·æ•°|å¸¸å‹¤|éå¸¸å‹¤ï¼‰", "å‰å¹´åº¦ï¼‘æ—¥å¹³å‡å¤–æ¥æ‚£è€…æ•°", "ç·¯åº¦", "çµŒåº¦", "page"]
FLAG = startCityIndex == 0 and startClinicIndex == 0 and startPage == 1 and startStoreIndex == 0
csvlist_header = set_columns(FLAG, args)
csvlist = []

click_search_with_medical_dep(driver)
time.sleep(WAIT_SEC + np.random.rand()*WAIT_SEC)  
select_city_buttons(driver)

for cityIndex in range(startCityIndex, nCities):
    nCities = select_city(driver, cityIndex)

    start_clinic_index = startClinicIndex if cityIndex == startCityIndex else 0
    for clinicIndex in range(start_clinic_index, len(TARGET_CLINIC)):
        select_clinic_type(driver, clinicIndex)
        search_button(driver)
        
        ## initial paging
        for i in range(startPage - 1):
            flag, page = paging(driver, page)
            time.sleep(WAIT_SEC + np.random.rand()*WAIT_SEC)
          
        while True:
            ## get list info
            nClinics, latlon_list = get_page_info(driver) 
            start_store_index = startStoreIndex if page == startPage and clinicIndex == startClinicIndex and cityIndex == startCityIndex else 0
            
            for storeIndex in range(start_store_index, nClinics):                
                # scrape info
                time.sleep(WAIT_SEC + np.random.rand()*WAIT_SEC)  
                visit_stores(driver, storeIndex)
                original_window = switch_window(driver)
                
                basic_info = scrape_basic_info(driver)
                service_info = scrape_clinic_service(driver)
                result_info = scrape_result_info(driver)
                _row = basic_info + service_info + result_info + latlon_list[storeIndex] + [page]

                #Close the tab or window
                driver.close()
                driver.switch_to.window(original_window)

                ## store data
                csvlist.append(_row)

                ## record
                if FLAG:
                    FLAG = False
                    write_to_csv(EXPORT_PATH, SOURCE_NAME, dt_now, page, csvlist_header)
                else:
                    pass

                write_to_csv(EXPORT_PATH, SOURCE_NAME, dt_now, page, csvlist)
                csvlist = []

                latestCityIndex = cityIndex # min 0
                latestClinicIndex = clinicIndex # min 0
                latestPage = page # min 1
                latestStore = storeIndex # min 0

            #paging
            flag, page = paging(driver, page)

            if flag:
                pass
            else:
                print("going to the next category...")
                break

print("done")
driver.close


# In[ ]:




