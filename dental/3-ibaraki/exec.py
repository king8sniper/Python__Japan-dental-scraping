from bs4 import BeautifulSoup
import requests
import csv
import jaconv
import re
import json
import csv
import time
import datetime
from dateutil.parser import parse
from normalize_japanese_addresses import normalize
import numpy as np





base_url='https://www.ibaraki-medinfo.jp/details/'

f = open("urls.txt", "r")

data=['timestamp', 'storename', 'address_original', 'address_normalize[0]', 'address_normalize[1]', 'æœ€çµ‚æ›´æ–°æ—¥', 'url', 
      'é–‹è¨­è€…ç¨®åˆ¥', 'é–‹è¨­è€…å', 'ç®¡ç†è€…å', 'æ­¯ç§‘ä¸€èˆ¬é ˜åŸŸä¸€è¦§', 'æ­¯ç§‘å£è…”å¤–ç§‘é ˜åŸŸä¸€è¦§', 'å°å…æ­¯ç§‘é ˜åŸŸä¸€è¦§', 'çŸ¯æ­£æ­¯ç§‘é ˜åŸŸä¸€è¦§',
      'æ–½è¨­çŠ¶æ³ä¸€è¦§', 'å¯¾å¿œå¯èƒ½ï¾…éº»é…”æ²»ç™‚ä¸€è¦§', 'åœ¨å®…åŒ»ç™‚', 'é€£æºï¾‰æœ‰ç„¡', 'æ­¯ç§‘åŒ»å¸«(ç·æ•°|å¸¸å‹¤|éžå¸¸å‹¤)', 'æ­¯ç§‘æŠ€å·¥å£«(ç·æ•°|å¸¸å‹¤|éžå¸¸å‹¤)', 
      'æ­¯ç§‘åŠ©æ‰‹(ç·æ•°|å¸¸å‹¤|éžå¸¸å‹¤)', 'æ­¯ç§‘è¡›ç”Ÿå£«(ç·æ•°|å¸¸å‹¤|éžå¸¸å‹¤)', 'å‰å¹´åº¦1æ—¥å¹³å‡å¤–æ¥æ‚£è€…æ•°', 'ç·¯åº¦', 'çµŒåº¦', 'page', 'è¨ºç™‚ç§‘']

fc=open('ibaraki.csv', 'a', newline='',encoding='utf-8')
# Create a CSV writer object
writer = csv.writer(fc)
# Write the data to the CSV file
writer.writerow(data)




###  Functions

## Functions  predefined
# -------------------------------------------------------------------------------------
def _normalization(arg):
    """
    æ–‡å­—åˆ—ã®æ­£è¦åŒ–ã‚’è¡Œã†å†…éƒ¨é–¢æ•°ã€‚
    ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«ã€å…¨è§’ã‚’åŠè§’ã«ã€å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã€ä¸å¯è¦–æ–‡å­—ã‚‚å‰Šé™¤ã™ã‚‹ã€‚
    """

    try:
        # ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›
        try:
            result = jaconv.hira2kata(arg)
        except AttributeError:
            result = arg

        # å…¨è§’ã‚’åŠè§’ã«å¤‰æ›
        try:
            result = jaconv.z2h(result, digit=True, ascii=True)
        except AttributeError:
            result = result

        # å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›
        try:
            result = result.lower()
        except AttributeError:
            result = result

        # ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤
        try:
            result = _str_clean(result)
        except TypeError:
            result = result

    except:
        result = arg

    return result

# -------------------------------------------------------------------------------------
def normalization(arg):
    # print(2, arg)
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_normalization, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæžœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿åž‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _str_clean(arg):
    """
    æ–‡å­—åˆ—ã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """

    try:
        result = arg.strip()
    except:
        result = arg

    try:
        result = re.sub(r"\r|\n|\r\n|\u3000|\t|ã€€| |,", " ", result)
    except TypeError:
        result = result

    return result

# -------------------------------------------------------------------------------------
def str_clean(arg):
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_str_clean, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæžœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿åž‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _split_buildingName(arg):
    # print(1, arg)
    """
    å»ºç‰©åã‚’åˆ‡ã‚Šåˆ†ã‘ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """
    ## ãƒã‚¤ãƒ•ãƒ³ã®ä¸€èˆ¬åŒ–
    address = normalization(arg)
    hyphens = '-Ë—á…³á­¸â€â€‘â€’â€“â€”â€•âƒâ»âˆ’â–¬â”€â”âž–ãƒ¼ã…¡ï¹˜ï¹£ï¼ï½°ð„ð†‘áš€'
    address = re.sub("|".join(hyphens), "-", address)
    address = re.sub(r"([ï½±-ï¾])(-)",r"\1ï½°", address)

    ## ä¸ç›®ã€ç•ªåœ°ã€å·ãªã©ã§ä½¿ã‚ã‚Œã‚‹æ¼¢å­—ã®å®šç¾©
    chome_poplist = ["ï¾‰åˆ‡","ç”ºç›®","åœ°å‰²","ä¸ç›®","ä¸","çµ„","ç•ªç”º","ç•ªåœ°","ç•ªç›®","ç•ª","å·å®¤","å·","è¡—åŒº","ç”»åœ°"]
    chome_popset = r"|".join(chome_poplist)
    chome_holdlist = ["æ¡æ±","æ¡è¥¿","æ¡å—","æ¡åŒ—","æ¡é€š","æ¡","æ±","è¥¿","å—","åŒ—"]
    chome_holdset = r"|".join(chome_holdlist)
    chome_alllist = chome_popset + chome_holdset
    chome_allset = r"|".join(chome_alllist)

    ## separate address
    result = re.findall(re.compile(f"(.*\d\[{chome_allset}\]*)|(\D+\[-\d\]+)|(.*)"), address)

    ## convert kanji into hyphen
    result = [[re.sub(f"(\d+)({chome_popset})", r"\1-", "".join(t)) for t in tl] for tl in result]

    ## concat all
    result = ["".join(t) for t in result]
    result = "".join(result)

    ## special case handling (1ï¾‰3 1åŒº1)
    result = re.sub(r"([^ï½±ï½°ï¾])(ï¾‰|ï½°)(\d)", r"\1-\3", result)
    result = re.sub(r"(\d)(åŒº)(\d)", r"\1-\3", result)
    result = re.sub("--", "-", result)

    ## separate into [japanese] + [number + hyphen] chunks
    result = re.findall(re.compile(f"(\D+[-\d]+[{chome_holdset}]*[-\d]+)|(\D+[-\d]+)|(.*)"), result)
    result = [t for t in ["".join(tl) for tl in result] if t != ""]
    # print(3, result)
    ## merge [number + hyphen] chunks
    try:
        result = [result[0]] + ["".join(result[1:])]
    except:
        result = result

    # 2åˆ—ç›®ãŒå˜ç‹¬ã€Œf, éšŽã€ã®ã¨ãã€1åˆ—ç›®ã®æœ«å°¾æ•°ã‚’2åˆ—ç›®ã¸ç§»å‹•
    if re.fullmatch(r"f|éšŽ", result[1]):
        result[1] = "".join(re.compile(r"\d+$").findall(result[0])) + result[1]
        result[0] = re.sub(r"\d+$", "", result[0])

    # 2åˆ—ç›®ã§ã€éšŽæ•°ãŒç•ªåœ°ã¨çµåˆã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã¨ãã€éšŽæ•°ã‚’1æ¡ã¨ã¿ãªã—ã€æ®‹ã‚Šã®æ•°å­—ã‚’ç•ªåœ°ã¨ã—ã¦1åˆ—ç›®ã¸ç§»å‹•
    if (re.fullmatch(r"\D+", result[0]) or re.search(r"-$", result[0])) and re.match(r"(\d*)(\d)(f|éšŽ)(\d*)", result[1]):
        result[1] = re.sub(r"(\d*)(\d)(f|éšŽ)(\d*)", r"\1,\2\3\4", result[1])
        result[0] = result[0] + result[1][:result[1].find(",")]
        result[1] = result[1][result[1].find(",")+1:]

    # æœ«å°¾ã®ãƒã‚¤ãƒ•ãƒ³ã‚’å‰Šé™¤
    result[0] = re.sub(r"-+$", "", result[0])

    return result





def get_page(id, category):
  response = requests.get(base_url+category+'/sb/'+id)
  return response

def get_base_data(html):
  soup = BeautifulSoup(html, 'lxml')
  timeStamp = datetime.date.today()

  span_items = soup.find("h3", {"class": "kikanMidashi"}).findAll("span")
  baseData={
    'storename': span_items[0].text.replace('\u3000', ' '),
    'updated_at': span_items[1].text.split('æ›´æ–°æ—¥ï¼š')[1].split('\n')[0],
  }
  table_items=soup.findAll("table", {"class", "input_info"})
  baseData['address']=table_items[3].findAll('tr')[2].find('td').text.replace('\u3000', ' ')
  try:
     storeAddressNormalize = "".join(list(normalize(baseData['address']).values())[0:4])
     baseData['address_normalize_1'] = _split_buildingName(storeAddressNormalize)[0]
     baseData['address_normalize_2'] = _split_buildingName(storeAddressNormalize)[1]
  except:
     baseData['address_normalize_1']=baseData['address_normalize_2']="na"
  baseData['founder_type']=table_items[1].find('tr').find('td').text.replace('\u3000', ' ')
  baseData['founder_name']=table_items[1].findAll('tr')[2].find('td').text.replace('\u3000', ' ')
  baseData['admin_name']=table_items[2].findAll('tr')[1].find('td').text.replace('\u3000', ' ')

  try:
    department_trs = soup.find("table", {"id": "tblShinryouKamoku"}).find_all('tr')
    medical_department = "["
    for department_tr in department_trs:
      try:
        medical_department += (department_tr.find('td').text.replace('\u3000', ' ') + ", ")
      except:
        a = 0
    medical_department += "]"
  except:
    medical_department = "na"

  baseData['timestamp'] = timeStamp
  baseData['medical_department'] = medical_department
       
  return baseData

def get_amenity_data(html):
  soup = BeautifulSoup(html, 'lxml')
  true_elements=soup.findAll('td', string='æœ‰')
  
  if len(true_elements) > 0 :
    amenityData=[]
    for t_el in true_elements :
      parent_true=t_el.find_parent('tr')
      amenityData.append(parent_true.find('th').text.strip())
  else :
    amenityData='na'
  
  return amenityData

def get_actual_data(html):
  soup = BeautifulSoup(html, 'lxml')
  table_items=soup.findAll("table", {"class", "input_info"})
  dentist_element=table_items[0].find('div', string='æ­¯ç§‘åŒ»å¸«')
  actualData={}
  if not dentist_element is None:
    parent_dentist=dentist_element.find_parent('tr')
    den_total=parent_dentist.findAll('td')[1].find('div').text if parent_dentist.findAll('td')[1].find('div').text!='' else '-'
    den_full=parent_dentist.findAll('td')[2].find('div').text if parent_dentist.findAll('td')[2].find('div').text!='' else '-'
    den_part=parent_dentist.findAll('td')[3].find('div').text if parent_dentist.findAll('td')[3].find('div').text!='' else '-'
    actualData['dentist']=den_total+'|'+ den_full + '|' + den_part
  else:
    actualData['dentist']='na'

  hygienist_element=table_items[0].find('div', string='æ­¯ç§‘è¡›ç”Ÿå£«')
  if not hygienist_element is None:
    parent_hygienist=hygienist_element.find_parent('tr')
    hyg_total=parent_hygienist.findAll('td')[1].find('div').text if parent_hygienist.findAll('td')[1].find('div').text!='' else '-'
    hyg_full=parent_hygienist.findAll('td')[2].find('div').text if parent_hygienist.findAll('td')[2].find('div').text!='' else '-'
    hyg_part=parent_hygienist.findAll('td')[3].find('div').text if parent_hygienist.findAll('td')[3].find('div').text!='' else '-'
    actualData['dental_hygienist']= hyg_total+'|'+ hyg_full + '|' + hyg_part
  else:
    actualData['dental_hygienist']='na'

  day_element=table_items[len(table_items)-1].find('div', string='å‰å¹´åº¦ï¼‘æ—¥å¹³å‡æ‚£è€…æ•°')
  actualData['day_patients']='na'
  if not day_element is None:
    parent_day=day_element.find_parent('tr')
    actualData['day_patients']=parent_day.findAll('td')[1].find('div').text.split('äºº')[0] if parent_day.findAll('td')[1].find('div').text!='' else 'na'

  return actualData

def get_contents_data(html):
  soup = BeautifulSoup(html, 'lxml')
  contentsData={}

  general_div=soup.find("span", string="æ­¯ç§‘é ˜åŸŸ").find_parent('div')
  general_element=general_div.find_next_sibling('table').findAll('tr')
  if len(general_element) > 1:
    general_dentistry=[]
    for g_el in general_element[1:]:
      general_dentistry.append(g_el.find('td').text)
  else :
    general_dentistry='na'
  contentsData['general_dentistry']=general_dentistry

  oral_div=soup.find("span", string="å£è…”å¤–ç§‘é ˜åŸŸ").find_parent('div')
  oral_element=oral_div.find_next_sibling('table').findAll('tr')
  if len(oral_element) > 1:
    oral_surgery=[]
    for o_el in oral_element[1:]:
      oral_surgery.append(o_el.find('td').text)
  else :
    oral_surgery='na'
  contentsData['oral_surgery']=oral_surgery

  homecare_div=soup.find("span", string="å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹åœ¨å®…åŒ»ç™‚ï¼ˆåœ¨å®…åŒ»ç™‚ï¼‰").find_parent('div')
  homecare_element=homecare_div.find_next_sibling('table').findAll('tr')
  if len(homecare_element) > 1:
    homecare=[]
    for h_el in homecare_element:
      homecare.append(h_el.find('td').text)
  else :
    homecare='na'      
  contentsData['homecare']=homecare

  collaboration_div=soup.find("span", string="å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹åœ¨å®…åŒ»ç™‚ï¼ˆä»–æ–½è¨­ã¨ã®é€£æºï¼‰").find_parent('div')
  collaboration_element=collaboration_div.find_next_sibling('table').findAll('tr')
  if len(collaboration_element) > 1:
    collaboration=[]
    for c_el in collaboration_element:
      collaboration.append(c_el.find('td').text)
  else :
    collaboration='na'   
  contentsData['collaboration']=collaboration

  return contentsData

index = 0
for line in f :
  page=line.split(',')[0]
  id=line.split(',')[1]
  data={}
  baseInfo=get_page(id, 'BaseInfo')
  if baseInfo.status_code == 200:
    baseData=get_base_data(baseInfo.text)

  amenityInfo=get_page(id, 'Amenity')
  if amenityInfo.status_code == 200:
    amenityData=get_amenity_data(amenityInfo.text)

  actualInfo=get_page(id, 'Actual')
  if actualInfo.status_code == 200:
    actualData=get_actual_data(actualInfo.text)

  contentsInfo=get_page(id, 'Contents')
  if contentsInfo.status_code == 200:
    contentsData=get_contents_data(contentsInfo.text)

  data=[
    baseData['timestamp'],
    baseData['storename'],
    baseData['address'],
    baseData['address_normalize_1'],
    baseData['address_normalize_2'],
    baseData['updated_at'],
    base_url+'BaseInfo'+'/sb/'+id,
    baseData['founder_type'],
    baseData['founder_name'],
    baseData['admin_name'],
    contentsData['general_dentistry'],
    contentsData['oral_surgery'],
    'na',
    'na',
    amenityData,
    'na',
    contentsData['homecare'],
    contentsData['collaboration'],
    actualData['dentist'],
    'na',
    'na',
    actualData['dental_hygienist'],
    actualData['day_patients'],
    'na',
    'na',
    page,
    baseData['medical_department']
  ]
  writer.writerow(data)
  index += 1
  print(id, index)

fc.close()
