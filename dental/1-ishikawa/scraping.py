###  Import Modules

from bs4 import BeautifulSoup
import requests
import jaconv
import re
import json
import csv
import time
import datetime
from normalize_japanese_addresses import normalize
import numpy as np
import builtins


###  Global Variables

WAIT_SEC = 15
GetIds_Post_Url = "http://i-search.pref.ishikawa.jp/ajax/tableMake.php"
GetDetailInfo_Base_Url = "http://i-search.pref.ishikawa.jp/detail.php?rd_no="
Arg = ['timestamp', 'storename', 'address_original', 'address_normalize[0]', 'address_normalize[1]', 'æœ€çµ‚æ›´æ–°æ—¥', 'url', 'é–‹è¨­è€…ç¨®åˆ¥', 'é–‹è¨­è€…å', 'ç®¡ç†è€…å', 'æ­¯ç§‘ä¸€èˆ¬é ˜åŸŸä¸€è¦§', 'æ­¯ç§‘å£è…”å¤–ç§‘é ˜åŸŸä¸€è¦§', 'å°å…æ­¯ç§‘é ˜åŸŸä¸€è¦§', 'çŸ¯æ­£æ­¯ç§‘é ˜åŸŸä¸€è¦§','æ–½è¨­çŠ¶æ³ä¸€è¦§', 'å¯¾å¿œå¯èƒ½ï¾…éº»é…”æ²»ç™‚ä¸€è¦§', 'åœ¨å®…åŒ»ç™‚', 'é€£æºï¾‰æœ‰ç„¡', 'æ­¯ç§‘åŒ»å¸«(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'æ­¯ç§‘æŠ€å·¥å£«(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'æ­¯ç§‘åŠ©æ‰‹(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'æ­¯ç§‘è¡›ç”Ÿå£«(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'å‰å¹´åº¦1æ—¥å¹³å‡å¤–æ¥æ‚£è€…æ•°', 'ç·¯åº¦', 'çµŒåº¦', 'page', 'è¨ºç™‚ç§‘']




###  Functions

## Functions  predefined
# -------------------------------------------------------------------------------------
def _normalization(arg):
    """
    æ–‡å­—åˆ—ã®æ­£è¦åŒ–ã‚’è¡Œã†å†…éƒ¨é–¢æ•°ã€‚
    ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«ã€å…¨è§’ã‚’åŠè§’ã«ã€å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã€ä¸å¯è¦–æ–‡å­—ã‚‚å‰Šé™¤ã™ã‚‹ã€‚
    """

    try:
        # ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›
        try:
            result = jaconv.hira2kata(arg)
        except AttributeError:
            result = arg

        # å…¨è§’ã‚’åŠè§’ã«å¤‰æ›
        try:
            result = jaconv.z2h(result, digit=True, ascii=True)
        except AttributeError:
            result = result

        # å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›
        try:
            result = result.lower()
        except AttributeError:
            result = result

        # ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤
        try:
            result = _str_clean(result)
        except TypeError:
            result = result

    except:
        result = arg

    return result

# -------------------------------------------------------------------------------------
def normalization(arg):
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_normalization, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _str_clean(arg):
    """
    æ–‡å­—åˆ—ã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """

    try:
        result = arg.strip()
    except:
        result = arg

    try:
        result = re.sub(r"\r|\n|\r\n|\u3000|\t|ã€€| |,", " ", result)
    except TypeError:
        result = result

    return result

# -------------------------------------------------------------------------------------
def str_clean(arg):
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_str_clean, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _split_buildingName(arg):
    """
    å»ºç‰©åã‚’åˆ‡ã‚Šåˆ†ã‘ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """
    ## ãƒã‚¤ãƒ•ãƒ³ã®ä¸€èˆ¬åŒ–
    address = normalization(arg)
    hyphens = '-Ë—á…³á­¸â€â€‘â€’â€“â€”â€•âƒâ»âˆ’â–¬â”€â”â–ãƒ¼ã…¡ï¹˜ï¹£ï¼ï½°ğ„ğ†‘áš€'
    address = re.sub("|".join(hyphens), "-", address)
    address = re.sub(r"([ï½±-ï¾])(-)",r"\1ï½°", address)

    ## ä¸ç›®ã€ç•ªåœ°ã€å·ãªã©ã§ä½¿ã‚ã‚Œã‚‹æ¼¢å­—ã®å®šç¾©
    chome_poplist = ["ï¾‰åˆ‡","ç”ºç›®","åœ°å‰²","ä¸ç›®","ä¸","çµ„","ç•ªç”º","ç•ªåœ°","ç•ªç›®","ç•ª","å·å®¤","å·","è¡—åŒº","ç”»åœ°"]
    chome_popset = r"|".join(chome_poplist)
    chome_holdlist = ["æ¡æ±","æ¡è¥¿","æ¡å—","æ¡åŒ—","æ¡é€š","æ¡","æ±","è¥¿","å—","åŒ—"]
    chome_holdset = r"|".join(chome_holdlist)
    chome_alllist = chome_popset + chome_holdset
    chome_allset = r"|".join(chome_alllist)

    ## separate address
    result = re.findall(re.compile(f"(.*\d\[{chome_allset}\]*)|(\D+\[-\d\]+)|(.*)"), address)

    ## convert kanji into hyphen
    result = [[re.sub(f"(\d+)({chome_popset})", r"\1-", "".join(t)) for t in tl] for tl in result]

    ## concat all
    result = ["".join(t) for t in result]
    result = "".join(result)

    ## special case handling (1ï¾‰3 1åŒº1)
    result = re.sub(r"([^ï½±ï½°ï¾])(ï¾‰|ï½°)(\d)", r"\1-\3", result)
    result = re.sub(r"(\d)(åŒº)(\d)", r"\1-\3", result)
    result = re.sub("--", "-", result)

    ## separate into [japanese] + [number + hyphen] chunks
    result = re.findall(re.compile(f"(\D+[-\d]+[{chome_holdset}]*[-\d]+)|(\D+[-\d]+)|(.*)"), result)
    result = [t for t in ["".join(tl) for tl in result] if t != ""]

    ## merge [number + hyphen] chunks
    try:
        result = [result[0]] + ["".join(result[1:])]
    except:
        result = result

    # 2åˆ—ç›®ãŒå˜ç‹¬ã€Œf, éšã€ã®ã¨ãã€1åˆ—ç›®ã®æœ«å°¾æ•°ã‚’2åˆ—ç›®ã¸ç§»å‹•
    if re.fullmatch(r"f|éš", result[1]):
        result[1] = "".join(re.compile(r"\d+$").findall(result[0])) + result[1]
        result[0] = re.sub(r"\d+$", "", result[0])

    # 2åˆ—ç›®ã§ã€éšæ•°ãŒç•ªåœ°ã¨çµåˆã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã¨ãã€éšæ•°ã‚’1æ¡ã¨ã¿ãªã—ã€æ®‹ã‚Šã®æ•°å­—ã‚’ç•ªåœ°ã¨ã—ã¦1åˆ—ç›®ã¸ç§»å‹•
    if (re.fullmatch(r"\D+", result[0]) or re.search(r"-$", result[0])) and re.match(r"(\d*)(\d)(f|éš)(\d*)", result[1]):
        result[1] = re.sub(r"(\d*)(\d)(f|éš)(\d*)", r"\1,\2\3\4", result[1])
        result[0] = result[0] + result[1][:result[1].find(",")]
        result[1] = result[1][result[1].find(",")+1:]

    # æœ«å°¾ã®ãƒã‚¤ãƒ•ãƒ³ã‚’å‰Šé™¤
    result[0] = re.sub(r"-+$", "", result[0])

    return result


## Functions  before receiving detail info
# -------------------------------------------------------------------------------------
def make_csv_file():
    Today = datetime.today().strftime('%Y-%m-%d')
    csv_file_name = "ishikawa" + str(Today) + ".csv"
    csv_file = open(csv_file_name, 'a', newline="", encoding="utf-8", errors="replace")
    writer = csv.writer(csv_file)
    writer.writerow(Arg)

# -------------------------------------------------------------------------------------
def get_clinic_ids():
    simple_file = open("simple.txt", "ab")

    for x in range(1, 50):
        page_num = x
        body = {"sr_mode": "13", "sr_opt_outmode": "1", "sr_area_base": "0", "vl_page": page_num}
        response = requests.post(GetIds_Post_Url, data = body, timeout = 20).content
        simple_file.write(response)
        time.sleep(5)
    simple_file.close()
    time.sleep(5)

    clinic_ids = []
    pattern = re.compile(r'no:([^,]+)')
    with open('simple.txt', encoding='utf-8') as f:
        text_str = f.read()

    for id in re.finditer(pattern, text_str):
        no = int((id.group(1)).strip("'"))
        clinic_ids.append(no)

    return clinic_ids

# -------------------------------------------------------------------------------------
def get_detailHtml(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.8) Gecko/20100722 Firefox/3.6.8 GTB7.1 (.NET CLR 3.5.30729)", "Referer": "http://example.com"}
    try:
        response = requests.get(url,headers=headers, timeout = 5)
        return response.text
    except requests.exceptions.ConnectTimeout:
        time.sleep(5 + np.random.rand()*5)
        response = requests.get(url,headers=headers, timeout = 5)
        return response.text


## Functions  after receiving detail info
# -------------------------------------------------------------------------------------
def get_base_data(Detail_Info):
    baseData = {}
    html_info1 = BeautifulSoup(Detail_Info, 'lxml')
    html_info = str(BeautifulSoup(Detail_Info, 'lxml'))

    timeStamp = datetime.date.today()
    try:
        storename = html_info1.find('div', {'id': 'basicname'}).find('h2', {'class': 'r21'}).text.replace('\u3000', ' ')
    except:
        storename = "na"
    try:
        address_original = html_info1.find('div', {'class': 'detail001'}).find("th", string="æ‰€åœ¨åœ°").find_next_sibling('td').text.strip().replace('\u3000', ' ').replace("çŸ³å·çœŒ","")
        # address_original = address_original.replace("çŸ³å·çœŒ","")
    except:
        address_original = "na"
    try:
        original = "çŸ³å·çœŒ" + address_original
        storeAddressNormalize = "".join(list(normalize(address_original).values())[0:4])
        address_normalize_1 = _split_buildingName(storeAddressNormalize)[0]
        address_normalize_2 = _split_buildingName(storeAddressNormalize)[1]
    except:
        address_normalize_1 = address_normalize_2 = "na"
    updateDate = "na"


    tr_tags = html_info1.find('div', {'class': 'detail001'}).find_all('tr')
    try:
        founder_text = tr_tags[2].find("td").text.strip()
        check_type = "åŒ»ç™‚æ³•äºº" in founder_text
        if check_type:
            founder_type = "åŒ»ç™‚æ³•äºº"
            founder_name = founder_text.replace("åŒ»ç™‚æ³•äººç¤¾å›£","").replace('\u3000', ' ')
        else :
            founder_type = "å€‹äºº"
            founder_name = founder_text.replace('\u3000', ' ')
    except:
        founder_type = "na"
        founder_name = "na"

    try:
        admin_text = tr_tags[3].find("th").text.strip()
        if admin_text == "æ³•äººä»£è¡¨è€…" :
            admin_name = tr_tags[4].find("td").text.strip().replace('\u3000', ' ')
        else :
            admin_name = tr_tags[3].find("td").text.strip().replace('\u3000', ' ')
    except:
        admin_name = "na"

    longitude = ''
    latitude = ''

    try:
        soup = BeautifulSoup(Detail_Info, 'html.parser')
        script_tag_x = soup.find('script', string=lambda t: t and 'var x' in t)
        script_tag_y = soup.find('script', string=lambda t: t and 'var y' in t)

        longitude = script_tag_x.text.split('var x')[1].split(';')[0].strip().strip('=').strip("'").strip('"')
        latitude = script_tag_y.text.split('var y')[1].split(';')[0].strip().strip('=').strip("'").strip('"')
    except:
        longitude = "na"
        latitude = "na"


    baseData['timestamp'] = timeStamp
    baseData['storename'] = storename
    baseData['address_original'] = address_original
    baseData['address_normalize[0]'] = address_normalize_1
    baseData['address_normalize[1]'] = address_normalize_2
    baseData['updateDate'] = updateDate
    baseData['founder_type'] = founder_type
    baseData['founder_name'] = founder_name
    baseData['admin_name'] = admin_name
    baseData['longitude'] = longitude
    baseData['latitude'] = latitude

    return baseData

# -------------------------------------------------------------------------------------
def get_clinic_data(Detail_Info):
    clinicData = {}
    html_info = BeautifulSoup(Detail_Info, 'lxml')

    try:
        general_td = html_info.find("td", string="æ­¯ç§‘")
        general_dentistry = general_td.find_next_sibling('td').text.strip().replace("ã€€", "").replace("\n", "")
    except:
        general_dentistry = "na"
    try:
        general_td = html_info.find("h3", string="å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç–¾æ‚£ãƒ»æ²»ç™‚å†…å®¹").find_parent('div')
        li_tags = general_td.find_next_sibling('div', {'class': 'detail034'}).find("th", string="æ­¯ç§‘é ˜åŸŸ").find_next_sibling('td').find_all('li')
        general_dentistry_list = "["
        for li_tag in li_tags:
            general_dentistry_list += (li_tag.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        general_dentistry_list += (general_dentistry + "]")
    except:
        general_dentistry_list = general_dentistry

    try:
        general_td = html_info.find("td", string="æ­¯ç§‘å£è…”å¤–ç§‘")
        oral_surgery = general_td.find_next_sibling('td').text.strip().replace("ã€€", "").replace("\n", "")
    except:
        oral_surgery = "na"
    try:
        general_td = html_info.find("h3", string="å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç–¾æ‚£ãƒ»æ²»ç™‚å†…å®¹").find_parent('div')
        li_tags = general_td.find_next_sibling('div', {'class': 'detail034'}).find("th", string="å£è…”å¤–ç§‘é ˜åŸŸ").find_next_sibling('td').find_all('li')
        oral_surgery_list = "["
        for li_tag in li_tags:
            oral_surgery_list += (li_tag.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        oral_surgery_list += (oral_surgery + "]")
    except:
        oral_surgery_list = oral_surgery

    try:
        general_td = html_info.find("td", string="å°å…æ­¯ç§‘")
        pediatric_dentistry = general_td.find_next_sibling('td').text.strip().replace("ã€€", "").replace("\n", "")
        if pediatric_dentistry == "ç„¡":
            pediatric_dentistry = "na"
    except:
        pediatric_dentistry = "na"

    try:
        general_td = html_info.find("td", string="çŸ¯æ­£æ­¯ç§‘")
        orthodontic_dentistry = general_td.find_next_sibling('td').text.strip().replace("ã€€", "").replace("\n", "")
        if orthodontic_dentistry == "ç„¡":
            orthodontic_dentistry = "na"
    except:
        orthodontic_dentistry = "na"

    try:
        general_td = html_info.find("th", string="æœ‰ã—ã¦ã„ã‚‹æ§‹é€ ")
        structure_list = general_td.find_next_sibling('td').find_all("li")
        has_structure = "["
        for s_list in structure_list:
            has_structure += (s_list.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        has_structure += "]"
    except:
        has_structure = "na"

    try:
        general_td = html_info.find("h3", string="å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç–¾æ‚£ãƒ»æ²»ç™‚å†…å®¹").find_parent('div')
        li_tags = general_td.find_next_sibling('div', {'class': 'detail034'}).find("th", string="éº»é…”é ˜åŸŸ").find_next_sibling('td').find_all('li')
        avariable_treatment = "["
        for li_tag in li_tags:
            avariable_treatment += (li_tag.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        avariable_treatment += "]"
    except:
        avariable_treatment = "na"

    try:
        general_td = html_info.find("th", string="åœ¨å®…åŒ»ç™‚")
        home_care_list = general_td.find_next_sibling('td').find_all("li")
        home_care = "["
        for s_list in home_care_list:
            home_care += (s_list.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        home_care += "]"
    except:
        home_care = "na"

    try:
        general_td = html_info.find("th", string="é€£æºã®æœ‰ç„¡")
        affiliate_check_list = general_td.find_next_sibling('td').find_all("li")
        affiliate_check = "["
        for s_list in affiliate_check_list:
            affiliate_check += (s_list.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        affiliate_check += "]"
    except:
        affiliate_check = "na"

    try:
        general_div = html_info.find("h3", string="è¨ºç™‚ç§‘ç›®").find_parent('div')
        td_tags = general_div.find_next_sibling('div', {'class': 'detail006'}).find_all('td', {'class': 'detail006bg'})
        medical_department_list = "["
        for td_tag in td_tags:
            medical_department_list += (td_tag.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        medical_department_list += "]"
    except:
        medical_department_list = "na"



    clinicData['general_dentistry'] = general_dentistry_list
    clinicData['oral_surgery'] = oral_surgery_list
    clinicData['pediatric_dentistry'] = pediatric_dentistry
    clinicData['orthodontic_dentistry'] = orthodontic_dentistry
    clinicData['has_structure'] = has_structure
    clinicData['avariable_treatment'] = avariable_treatment
    clinicData['home_care'] = home_care
    clinicData['affiliate_check'] = affiliate_check
    clinicData['medical_department'] = medical_department_list

    return clinicData

# -------------------------------------------------------------------------------------
def get_person_data(Detail_Info):
    personData = {}
    html_info = BeautifulSoup(Detail_Info, 'lxml')

    try:
        dentist_div = html_info.find("td", string="æ­¯ç§‘åŒ»å¸«æ•°")
        dentist_element = dentist_div.find_next_sibling('td').text.strip().replace('äºº', '')
    except:
        dentist_element = "na"

    try:
        technician_div = html_info.find("td", string="æ­¯ç§‘æŠ€å·¥å£«æ•°")
        dental_technician = technician_div.find_next_sibling('td').text.strip().replace('äºº', '')
    except:
        dental_technician = "na"

    try:
        assistant_div = html_info.find("td", string="æ­¯ç§‘åŠ©æ‰‹æ•°")
        dental_assistant = assistant_div.find_next_sibling('td').text.strip().replace('äºº', '')
    except:
        dental_assistant = "na"

    try:
        hygienist_div = html_info.find("td", string="æ­¯ç§‘è¡›ç”Ÿå£«æ•°")
        dental_hygienist_element = hygienist_div.find_next_sibling('td').text.strip().replace('äºº', '')
    except:
        dental_hygienist_element = "na"

    try:
        people_count_div = html_info.find("td", string="å¤–æ¥æ‚£è€…æ•°")
        average_people_count = people_count_div.find_next_sibling('td').text.strip().replace('äºº', '')
    except:
        average_people_count = "na"


    personData['dentist'] = dentist_element + '|-|-'
    personData['dental_technician'] = dental_technician + '|-|-'
    personData['dental_assistant'] = dental_assistant + '|-|-'
    personData['dental_hygienist'] = dental_hygienist_element + '|-|-'
    personData['average_people_count'] = average_people_count

    return personData


## Init Function
# -------------------------------------------------------------------------------------
def init():
    datetime_module = builtins.__import__('datetime')
    Today = datetime_module.date.today()

    csv_file_name = "ishikawa" + str(Today) + ".csv"
    csv_file = open(csv_file_name, 'a', newline="", encoding="utf-8", errors="replace")
    writer = csv.writer(csv_file)
    writer.writerow(Arg)
    clinic_ids = get_clinic_ids()
    print(len(clinic_ids)) #[139:] + clinic_ids[:139]

    index = 0
    for clinic_id in clinic_ids:
        detail_url = GetDetailInfo_Base_Url + str(clinic_id)
        print(detail_url)

        Detail_Info = get_detailHtml(detail_url)
        baseData = get_base_data(Detail_Info)
        baseData['url'] = detail_url
        clinicData = get_clinic_data(Detail_Info)
        personData = get_person_data(Detail_Info)
        index += 1
        page = (int(index/10)+1)

        data=[
            baseData['timestamp'],
            baseData['storename'],
            baseData['address_original'],
            baseData['address_normalize[0]'],
            baseData['address_normalize[1]'],
            baseData['updateDate'],
            baseData['url'],
            baseData['founder_type'],
            baseData['founder_name'],
            baseData['admin_name'],
            clinicData['general_dentistry'],
            clinicData['oral_surgery'],
            clinicData['pediatric_dentistry'],
            clinicData['orthodontic_dentistry'],
            clinicData['has_structure'],
            clinicData['avariable_treatment'],
            clinicData['home_care'],
            clinicData['affiliate_check'],
            personData['dentist'],
            personData['dental_technician'],
            personData['dental_assistant'],
            personData['dental_hygienist'],
            personData['average_people_count'],
            baseData['latitude'],
            baseData['longitude'],
            page,
            clinicData['medical_department']
        ]


        writer.writerow(data)
        print(index, clinic_id)
        # print(data)

    csv_file.close()

init()

