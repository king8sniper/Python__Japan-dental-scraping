###  Import Modules

from bs4 import BeautifulSoup
import requests
import jaconv
import re
import json
import csv
import time
import datetime
from dateutil.parser import parse
from normalize_japanese_addresses import normalize
import numpy as np
import builtins


###  Global Variables

WAIT_SEC = 15
Arg = ['timestamp', 'storename', 'address_original', 'address_normalize[0]', 'address_normalize[1]', 'æœ€çµ‚æ›´æ–°æ—¥', 'url', 'é–‹è¨­è€…ç¨®åˆ¥', 'é–‹è¨­è€…å', 'ç®¡ç†è€…å', 'æ­¯ç§‘ä¸€èˆ¬é ˜åŸŸä¸€è¦§', 'æ­¯ç§‘å£è…”å¤–ç§‘é ˜åŸŸä¸€è¦§', 'å°å…æ­¯ç§‘é ˜åŸŸä¸€è¦§', 'çŸ¯æ­£æ­¯ç§‘é ˜åŸŸä¸€è¦§','æ–½è¨­çŠ¶æ³ä¸€è¦§', 'å¯¾å¿œå¯èƒ½ï¾…éº»é…”æ²»ç™‚ä¸€è¦§', 'åœ¨å®…åŒ»ç™‚', 'é€£æºï¾‰æœ‰ç„¡', 'æ­¯ç§‘åŒ»å¸«(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'æ­¯ç§‘æŠ€å·¥å£«(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'æ­¯ç§‘åŠ©æ‰‹(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'æ­¯ç§‘è¡›ç”Ÿå£«(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'å‰å¹´åº¦1æ—¥å¹³å‡å¤–æ¥æ‚£è€…æ•°', 'ç·¯åº¦', 'çµŒåº¦', 'page', 'è¨ºç™‚ç§‘']

Get_ClinicId_BaseUrl = "https://iryojoho.pref.aichi.jp/medical/?dayofweek=&departmentcategoryid=10&languagelevel=%E2%97%8E&kenshin_keyword=&searchtype=function&gairai_keyword=&objecttype=1%2C2%2C4&requestpage="

GetInfo_Detail_BaseUrl = "https://iryojoho.pref.aichi.jp/medical/detail.cfm?objectid="
GetInfo_Access_BaseUrl = "https://iryojoho.pref.aichi.jp/medical/access.cfm?objectid="
GetInfo_Service_BaseUrl = "https://iryojoho.pref.aichi.jp/medical/service.cfm?objectid="
GetInfo_Cost_BaseUrl = "https://iryojoho.pref.aichi.jp/medical/cost.cfm?objectid="
GetInfo_Consult_BaseUrl = "https://iryojoho.pref.aichi.jp/medical/consult.cfm?objectid="
GetInfo_Showcase_BaseUrl = "https://iryojoho.pref.aichi.jp/medical/showcase.cfm?objectid="





###  Functions

## Functions  predefined
# -------------------------------------------------------------------------------------
def _normalization(arg):
    """
    æ–‡å­—åˆ—ã®æ­£è¦åŒ–ã‚’è¡Œã†å†…éƒ¨é–¢æ•°ã€‚
    ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«ã€å…¨è§’ã‚’åŠè§’ã«ã€å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã€ä¸å¯è¦–æ–‡å­—ã‚‚å‰Šé™¤ã™ã‚‹ã€‚
    """

    try:
        # ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›
        try:
            result = jaconv.hira2kata(arg)
        except AttributeError:
            result = arg

        # å…¨è§’ã‚’åŠè§’ã«å¤‰æ›
        try:
            result = jaconv.z2h(result, digit=True, ascii=True)
        except AttributeError:
            result = result

        # å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›
        try:
            result = result.lower()
        except AttributeError:
            result = result

        # ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤
        try:
            result = _str_clean(result)
        except TypeError:
            result = result

    except:
        result = arg

    return result

# -------------------------------------------------------------------------------------
def normalization(arg):
    # print(2, arg)
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_normalization, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _str_clean(arg):
    """
    æ–‡å­—åˆ—ã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """

    try:
        result = arg.strip()
    except:
        result = arg

    try:
        result = re.sub(r"\r|\n|\r\n|\u3000|\t|ã€€| |,", " ", result)
    except TypeError:
        result = result

    return result

# -------------------------------------------------------------------------------------
def str_clean(arg):
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_str_clean, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _split_buildingName(arg):
    # print(1, arg)
    """
    å»ºç‰©åã‚’åˆ‡ã‚Šåˆ†ã‘ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """
    ## ãƒã‚¤ãƒ•ãƒ³ã®ä¸€èˆ¬åŒ–
    address = normalization(arg)
    hyphens = '-Ë—á…³á­¸â€â€‘â€’â€“â€”â€•âƒâ»âˆ’â–¬â”€â”â–ãƒ¼ã…¡ï¹˜ï¹£ï¼ï½°ğ„ğ†‘áš€'
    address = re.sub("|".join(hyphens), "-", address)
    address = re.sub(r"([ï½±-ï¾])(-)",r"\1ï½°", address)

    ## ä¸ç›®ã€ç•ªåœ°ã€å·ãªã©ã§ä½¿ã‚ã‚Œã‚‹æ¼¢å­—ã®å®šç¾©
    chome_poplist = ["ï¾‰åˆ‡","ç”ºç›®","åœ°å‰²","ä¸ç›®","ä¸","çµ„","ç•ªç”º","ç•ªåœ°","ç•ªç›®","ç•ª","å·å®¤","å·","è¡—åŒº","ç”»åœ°"]
    chome_popset = r"|".join(chome_poplist)
    chome_holdlist = ["æ¡æ±","æ¡è¥¿","æ¡å—","æ¡åŒ—","æ¡é€š","æ¡","æ±","è¥¿","å—","åŒ—"]
    chome_holdset = r"|".join(chome_holdlist)
    chome_alllist = chome_popset + chome_holdset
    chome_allset = r"|".join(chome_alllist)

    ## separate address
    result = re.findall(re.compile(f"(.*\d\[{chome_allset}\]*)|(\D+\[-\d\]+)|(.*)"), address)

    ## convert kanji into hyphen
    result = [[re.sub(f"(\d+)({chome_popset})", r"\1-", "".join(t)) for t in tl] for tl in result]

    ## concat all
    result = ["".join(t) for t in result]
    result = "".join(result)

    ## special case handling (1ï¾‰3 1åŒº1)
    result = re.sub(r"([^ï½±ï½°ï¾])(ï¾‰|ï½°)(\d)", r"\1-\3", result)
    result = re.sub(r"(\d)(åŒº)(\d)", r"\1-\3", result)
    result = re.sub("--", "-", result)

    ## separate into [japanese] + [number + hyphen] chunks
    result = re.findall(re.compile(f"(\D+[-\d]+[{chome_holdset}]*[-\d]+)|(\D+[-\d]+)|(.*)"), result)
    result = [t for t in ["".join(tl) for tl in result] if t != ""]
    # print(3, result)
    ## merge [number + hyphen] chunks
    try:
        result = [result[0]] + ["".join(result[1:])]
    except:
        result = result

    # 2åˆ—ç›®ãŒå˜ç‹¬ã€Œf, éšã€ã®ã¨ãã€1åˆ—ç›®ã®æœ«å°¾æ•°ã‚’2åˆ—ç›®ã¸ç§»å‹•
    if re.fullmatch(r"f|éš", result[1]):
        result[1] = "".join(re.compile(r"\d+$").findall(result[0])) + result[1]
        result[0] = re.sub(r"\d+$", "", result[0])

    # 2åˆ—ç›®ã§ã€éšæ•°ãŒç•ªåœ°ã¨çµåˆã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã¨ãã€éšæ•°ã‚’1æ¡ã¨ã¿ãªã—ã€æ®‹ã‚Šã®æ•°å­—ã‚’ç•ªåœ°ã¨ã—ã¦1åˆ—ç›®ã¸ç§»å‹•
    if (re.fullmatch(r"\D+", result[0]) or re.search(r"-$", result[0])) and re.match(r"(\d*)(\d)(f|éš)(\d*)", result[1]):
        result[1] = re.sub(r"(\d*)(\d)(f|éš)(\d*)", r"\1,\2\3\4", result[1])
        result[0] = result[0] + result[1][:result[1].find(",")]
        result[1] = result[1][result[1].find(",")+1:]

    # æœ«å°¾ã®ãƒã‚¤ãƒ•ãƒ³ã‚’å‰Šé™¤
    result[0] = re.sub(r"-+$", "", result[0])

    return result


## Functions  get Dental Clinic Ids

# -------------------------------------------------------------------------------------
def get_ids_html(page):
    response = requests.get(page, timeout = 5)
    if response.status_code == 200:
        return response
    else:
        get_ids_html(page)

# -------------------------------------------------------------------------------------
def save_clinic_ids(clinic_ids_html):
    html_info = BeautifulSoup(clinic_ids_html, 'lxml')
    id_list_html = html_info.find_all('li', {'class': 'detail-name'})
    clinicid_file = open("Clinicids.txt", "ab")

    for id_html in id_list_html:
        match = re.search(r'objectid=(\d+)', str(id_html))
        if match:
            ClinicIds = match.group(1)
            cid_str = (str(ClinicIds) + ", ").encode("utf-8")
            clinicid_file.write(cid_str)
        else:
            print('No match found')
    clinicid_file.close()

# -------------------------------------------------------------------------------------
def get_clinic_ids():

    for page in range(1, 184):
        ClinicId_pageUrl = Get_ClinicId_BaseUrl + str(page)
        clinic_ids_html = get_ids_html(ClinicId_pageUrl)
        if clinic_ids_html.status_code == 200:
            print(page)
            get_ids = save_clinic_ids(clinic_ids_html.text)
# get_clinic_ids()


def rerutn_clinic_ids():
    clinic_ids = []
    with open('Clinicids.txt', 'r') as file:
        content = file.read()
    clinic_ids = content.split(", ")
    # print(clinic_ids)
    return clinic_ids


## Functions  get Dental Clinic Info

# -------------------------------------------------------------------------------------
def get_info_html(page):
    response = requests.get(page, timeout = 20)
    if response.status_code == 200:
        return response
    else:
        get_info_html(page)

def get_detail_info(detailHtml):

    detail_data = {}
    html_info = BeautifulSoup(detailHtml, 'lxml')
    timeStamp = datetime.date.today()

    try:
        storename = html_info.find('table', {'aria-label': 'åŒ»ç™‚æ©Ÿé–¢ã®åç§°'}).find("th", string="æ©Ÿé–¢åç§°").find_next_sibling('td').text.strip().replace('\u3000', ' ')
    except:
        storename = "na"
    try:
        address_original = html_info.find('table', {'aria-label': 'åŒ»ç™‚æ©Ÿé–¢ã®æ‰€åœ¨åœ°'}).find("th", string="æ‰€åœ¨åœ°").find_next_sibling('td').text.strip().replace('\u3000', ' ').replace("æ„›çŸ¥çœŒ","")
    except:
        address_original = "na"
    try:
        original = "æ„›çŸ¥çœŒ" + address_original
        storeAddressNormalize = "".join(list(normalize(address_original).values())[0:4])
        address_normalize_1 = _split_buildingName(storeAddressNormalize)[0]
        address_normalize_2 = _split_buildingName(storeAddressNormalize)[1]
    except:
        address_normalize_1 = address_normalize_2 = "na"


    try:
        founder_text = html_info.find('table', {'aria-label': 'åŒ»ç™‚æ©Ÿé–¢ã®é–‹è¨­è€…'}).find("th", string="é–‹è¨­è€…åç§°").find_next_sibling('td').text
        check_type = "æ³•äºº" in founder_text
        if check_type:
            founder_type = founder_text[:founder_text.index('æ³•äºº')].replace("\n", "").replace("\t", "") + "æ³•äºº"
            founder_name = founder_text[founder_text.index('æ³•äºº') + 2:].strip().replace('\u3000', ' ')
        else :
            founder_type = "å€‹äºº"
            founder_name = founder_text.strip().replace('\u3000', ' ')
    except:
        founder_type = "na"
        founder_name = "na"
    try:
        admin_name = html_info.find('table', {'aria-label': 'åŒ»ç™‚æ©Ÿé–¢ã®ç®¡ç†è€…'}).find("th", string="ç®¡ç†è€…åç§°").find_next_sibling('td').text.strip().replace('\u3000', ' ')
    except:
        admin_name = "na"
    try:
        date_str = html_info.find('div', {'id': 'contents'}).find("p", {'class': 'text-right'}).text.replace("æœ€çµ‚æ›´æ–°æ—¥ï¼š","")
        date_obj = parse(date_str)
        year_str = str(date_obj.year)
        month_str = str(date_obj.month)
        day_str = str(date_obj.day)
        updateDate = year_str + "å¹´" + month_str + "æœˆ" + day_str + "æ—¥"
    except:
        updateDate = "na"


    longitude = "na"
    latitude = "na"

    detail_data['timestamp'] = timeStamp
    detail_data['storename'] = storename
    detail_data['address_original'] = address_original
    detail_data['address_normalize[0]'] = address_normalize_1
    detail_data['address_normalize[1]'] = address_normalize_2
    detail_data['updateDate'] = updateDate
    detail_data['founder_type'] = founder_type
    detail_data['founder_name'] = founder_name
    detail_data['admin_name'] = admin_name
    detail_data['longitude'] = longitude
    detail_data['latitude'] = latitude

    return detail_data

def get_consult_info(consultHtml):
    consult_data = {}
    html_info = BeautifulSoup(consultHtml, 'lxml')

    try:
        general_dentistry = "["

        try:
            html_info.find("h3", string="æ­¯ç§‘é ˜åŸŸ").find_next_sibling('table').find('th')

            general_ths = html_info.find("h3", string="æ­¯ç§‘é ˜åŸŸ").find_next_sibling('table').find_all('th')
            for general_th in general_ths:
                if general_th.find_next_sibling('td').text.strip().strip(' ').replace("ã€€", "").replace("\n", "") == "â—‹":
                    general_dentistry += (general_th.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        except:
            general_tds = html_info.find("h3", string="æ­¯ç§‘é ˜åŸŸ").find_next_sibling('table').find_all('td')
            for general_td in general_tds:
                if general_td.text.strip() != "":
                    general_dentistry += (general_td.text.strip().replace("ã€€", "").replace("\n", "") + ", ")

        general_dentistry += "]"
    except:
        general_dentistry = "na"


    try:
        oral_surgery = "["

        try:
            html_info.find("h3", string="æ­¯ç§‘å£è…”(ãã†)å¤–ç§‘é ˜åŸŸ").find_next_sibling('table').find('th')

            general_ths = html_info.find("h3", string="æ­¯ç§‘å£è…”(ãã†)å¤–ç§‘é ˜åŸŸ").find_next_sibling('table').find_all('th')
            for general_th in general_ths:
                if general_th.find_next_sibling('td').text.strip().strip(' ').replace("ã€€", "").replace("\n", "") == "â—‹":
                    oral_surgery += (general_th.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        except:
            general_tds = html_info.find("h3", string="æ­¯ç§‘é ˜åŸŸ").find_next_sibling('table').find_all('td')
            for general_td in general_tds:
                if general_td.text.strip() != "":
                    oral_surgery += (general_td.text.strip().replace("ã€€", "").replace("\n", "") + ", ")

        oral_surgery += "]"
    except:
        oral_surgery = "na"


    pediatric_dentistry = "na"
    orthodontic_dentistry = "na"

    try:
        avariable_treatment = "["
        general_tds = html_info.find("h3", string="éº»é…”é ˜åŸŸ").find_next_sibling('table').find_all('td')
        for general_td in general_tds:
            if general_td.text.strip() != "":
                text_without_numbers = re.sub(r'\d+', '', general_td.text.strip())
                cleaned_text = text_without_numbers.replace("ã€€", "").replace("\n", "")
                avariable_treatment += (cleaned_text + ", ")
        avariable_treatment += "]"
    except:
        avariable_treatment = "na"

    try:
        home_care = "["
        general_tds = html_info.find("h3", string="åœ¨å®…ç™‚é¤ŠæŒ‡å°").find_next_sibling('table').find_all('td')
        for general_td in general_tds:
            home_care += (general_td.text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        home_care += "]"
    except:
        home_care = "na"

    affiliate_check = "na"


    try:
        medical_department_list = "["

        try:
            html_info.find("h2", string="å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç–¾æ‚£åˆã¯æ²»ç™‚ã®å†…å®¹").find_parent('section', {'class': 'around'})
            
            general_secs = html_info.find("h2", string="å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç–¾æ‚£åˆã¯æ²»ç™‚ã®å†…å®¹").find_parent('section').find_parent('div', {'id': 'details'}).find_all('section', {'class': 'around'})
            for general_sec in general_secs:
                medical_department_list += (general_sec.find('h3', {'class': 'quinary-title'}).text.strip().replace("ã€€", "").replace("\n", "") + ", ")
        except:
            general_h3s = html_info.find("h2", string="å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç–¾æ‚£åˆã¯æ²»ç™‚ã®å†…å®¹").find_parent('section').find_parent('div', {'id': 'details'}).find_all('h3')
            for general_h3 in general_h3s:
                medical_department_list += (general_h3.text.strip().replace("ã€€", "").replace("\n", "") + ", ")

        medical_department_list += "]"
    except:
        medical_department_list = "na"


    consult_data['general_dentistry'] = general_dentistry
    consult_data['oral_surgery'] = oral_surgery
    consult_data['pediatric_dentistry'] = pediatric_dentistry
    consult_data['orthodontic_dentistry'] = orthodontic_dentistry
    consult_data['avariable_treatment'] = avariable_treatment
    consult_data['home_care'] = home_care
    consult_data['affiliate_check'] = affiliate_check
    consult_data['medical_department'] = medical_department_list

    return consult_data

def get_access_info(accessHtml):
    access_data = {}
    html_info = BeautifulSoup(accessHtml, 'lxml')

    has_structure = ' '
    try:
        general_td = html_info.find('table', {'aria-label': 'åŒ»ç™‚æ©Ÿé–¢ã®é§è»Šå ´'}).find("th", string="é§è»Šå ´ã®æœ‰ç„¡ï¼ˆå¥‘ç´„é§è»Šå ´ã‚‚å«ã‚€ï¼‰").find_next_sibling('td')
        if general_td.text.strip() == "æœ‰ã‚Š":
            has_structure = "é§è»Šå ´ã®æœ‰ç„¡ï¼ˆå¥‘ç´„é§è»Šå ´ã‚‚å«ã‚€ï¼‰"
    except:
        has_structure = " "

    access_data['has_structure'] = has_structure

    return access_data

def get_service_info(serviceHtml):
    service_data = {}
    html_info = BeautifulSoup(serviceHtml, 'lxml')
    has_structure = []
    table_labels = ['å…¥é™¢é£Ÿã®æä¾›æ–¹æ³•', 'éšœå®³è€…ã«å¯¾ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹', 'è»Šæ¤…å­ç­‰åˆ©ç”¨è€…ã«å¯¾ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹', 'ç—…é™¢å†…ã®å£²åº—åˆã¯é£Ÿå ‚ã®æœ‰ç„¡', 'å—å‹•å–«ç…™ã‚’é˜²æ­¢ã™ã‚‹ãŸã‚ã®æªç½®']

    for table_label in table_labels:
        try:
            general_trs = html_info.find('table', {'aria-label': table_label}).find_all("tr")
            for general_tr in general_trs:
                if general_tr.find('td').text.strip().replace("ã€€", "").replace("\n", "") == "æœ‰ã‚Š":
                    has_structure.append(general_tr.find('th').text.strip().replace("ã€€", "").replace("\n", ""))
        except:
            a = 5

    service_data['has_structure'] = has_structure

    return service_data

def get_showcase_info(showcaseHtml):
    showcase_data = {}
    html_info = BeautifulSoup(showcaseHtml, 'lxml')

    dentist = "na"
    dental_hygienist = "na"
    try:
        general_trs = html_info.find('table', {'aria-label': 'åŒ»ç™‚æ©Ÿé–¢ã®äººå“¡é…ç½®'}).find('tbody').find_all("tr")
        for general_tr in general_trs:
            if general_tr.find('th').text.strip().replace("ã€€", "").replace("\n", "") == "æ­¯ç§‘åŒ»å¸«":
                dentist_all = general_tr.find_all('td')[0].text.strip().replace("ã€€", "").replace("\n", "")
                dentist_in = general_tr.find_all('td')[1].text.strip().replace("ã€€", "").replace("\n", "")
                dentist_out = general_tr.find_all('td')[2].text.strip().replace("ã€€", "").replace("\n", "")
                dentist = dentist_all + '|' + dentist_in + '|' + dentist_out
    except:
        dentist = "na"

    try:
        general_trs = html_info.find('table', {'aria-label': 'åŒ»ç™‚æ©Ÿé–¢ã®äººå“¡é…ç½®'}).find('tbody').find_all("tr")
        for general_tr in general_trs:
            if general_tr.find('th').text.strip().replace("ã€€", "").replace("\n", "") == "æ­¯ç§‘è¡›ç”Ÿå£«":
                hygienist_all = general_tr.find_all('td')[0].text.strip().replace("ã€€", "").replace("\n", "")
                hygienist_in = general_tr.find_all('td')[1].text.strip().replace("ã€€", "").replace("\n", "")
                hygienist_out = general_tr.find_all('td')[2].text.strip().replace("ã€€", "").replace("\n", "")
                dental_hygienist = hygienist_all + '|' + hygienist_in + '|' + hygienist_out
    except:
        dental_hygienist = "na"

    dental_technician = "na"
    dental_assistant = "na"

    try:
        average_people_count = html_info.find('table', {'aria-label': 'æ‚£è€…æ•°åŠã³å¹³å‡åœ¨é™¢æ—¥æ•°'}).find("th", string="å‰å¹´åº¦ï¼‘æ—¥å¹³å‡æ‚£è€…æ•°").find_next_sibling('td').text.strip().replace("ã€€", "").replace("\n", "")
    except:
        average_people_count = ""

    showcase_data['dentist'] = dentist
    showcase_data['dental_technician'] = dental_technician
    showcase_data['dental_assistant'] = dental_assistant
    showcase_data['dental_hygienist'] = dental_hygienist
    showcase_data['average_people_count'] = average_people_count

    return showcase_data


def init():
    get_clinic_ids()
    datetime_module = builtins.__import__('datetime')
    Today = datetime_module.date.today()

    csv_file_name = "aichi" + str(Today) + ".csv"
    csv_file = open(csv_file_name, 'a', newline="", encoding="utf-8", errors="replace")
    writer = csv.writer(csv_file)
    writer.writerow(Arg)

    clinic_ids = rerutn_clinic_ids()
    # clinic_ids = [51865, 51860]
    # clinic_ids = [51860]  [546:] + clinic_ids[:546]
    print(len(clinic_ids))

    index = 0
    for cid in clinic_ids:
        detailPage_url = GetInfo_Detail_BaseUrl + str(cid)
        consultPage_url = GetInfo_Consult_BaseUrl + str(cid)
        accessPage_url = GetInfo_Access_BaseUrl + str(cid)
        servicePage_url = GetInfo_Service_BaseUrl + str(cid)
        showcasePage_url = GetInfo_Showcase_BaseUrl + str(cid)
        # detailPage_url = GetInfo_Cost_BaseUrl + str(cid)


        get_detailData = get_info_html(detailPage_url)
        if get_detailData.status_code == 200:
            detailInfo = get_detail_info(get_detailData.text)
        
        get_consultData = get_info_html(consultPage_url)
        if get_consultData.status_code == 200:
            consultInfo = get_consult_info(get_consultData.text)

        get_accessData = get_info_html(accessPage_url)
        if get_accessData.status_code == 200:
            accessInfo = get_access_info(get_accessData.text)

        get_serviceData = get_info_html(servicePage_url)
        if get_serviceData.status_code == 200:
            serviceInfo = get_service_info(get_serviceData.text)

        get_showcaseData = get_info_html(showcasePage_url)
        if get_showcaseData.status_code == 200:
            showcaseInfo = get_showcase_info(get_showcaseData.text)

        has_structure = "["
        for li in serviceInfo['has_structure']:
            has_structure += (li + ", ")
        has_structure += (", " + accessInfo['has_structure'] + "]")
        index += 1
        page = (int(index / 20) + 1)

        data=[
            detailInfo['timestamp'],
            detailInfo['storename'],
            detailInfo['address_original'],
            detailInfo['address_normalize[0]'],
            detailInfo['address_normalize[1]'],
            detailInfo['updateDate'],
            detailPage_url,
            detailInfo['founder_type'],
            detailInfo['founder_name'],
            detailInfo['admin_name'],
            consultInfo['general_dentistry'],
            consultInfo['oral_surgery'],
            consultInfo['pediatric_dentistry'],
            consultInfo['orthodontic_dentistry'],
            has_structure,
            consultInfo['avariable_treatment'],
            consultInfo['home_care'],
            consultInfo['affiliate_check'],
            showcaseInfo['dentist'],
            showcaseInfo['dental_technician'],
            showcaseInfo['dental_assistant'],
            showcaseInfo['dental_hygienist'],
            showcaseInfo['average_people_count'],
            detailInfo['longitude'],
            detailInfo['latitude'],
            page,
            consultInfo['medical_department']
        ]

        writer.writerow(data)
        print(index, cid)
        print(detailPage_url)

    csv_file.close()





    # detail_file = open("showcase.html", "ab")
    # detail_file.write(get_showcaseData.content)
    # detail_file.close()

init()
