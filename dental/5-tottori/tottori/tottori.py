###  Import Modules

from bs4 import BeautifulSoup
import requests
import jaconv
import re
import json
import csv
import time
import datetime
from dateutil.parser import parse
from normalize_japanese_addresses import normalize
import numpy as np
import builtins


###  Global Variables

WAIT_SEC = 15
Arg = ['timestamp', 'storename', 'address_original', 'address_normalize[0]', 'address_normalize[1]', 
       'æœ€çµ‚æ›´æ–°æ—¥', 'url', 'é–‹è¨­è€…ç¨®åˆ¥', 'é–‹è¨­è€…å', 'ç®¡ç†è€…å', 'æ­¯ç§‘ä¸€èˆ¬é ˜åŸŸä¸€è¦§', 'æ­¯ç§‘å£è…”å¤–ç§‘é ˜åŸŸä¸€è¦§', 
       'å°å…æ­¯ç§‘é ˜åŸŸä¸€è¦§', 'çŸ¯æ­£æ­¯ç§‘é ˜åŸŸä¸€è¦§','æ–½è¨­çŠ¶æ³ä¸€è¦§', 'å¯¾å¿œå¯èƒ½ï¾…éº»é…”æ²»ç™‚ä¸€è¦§', 'åœ¨å®…åŒ»ç™‚', 'é€£æºï¾‰æœ‰ç„¡', 
       'æ­¯ç§‘åŒ»å¸«(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'æ­¯ç§‘æŠ€å·¥å£«(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'æ­¯ç§‘åŠ©æ‰‹(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 
       'æ­¯ç§‘è¡›ç”Ÿå£«(ç·æ•°|å¸¸å‹¤|éå¸¸å‹¤)', 'å‰å¹´åº¦1æ—¥å¹³å‡å¤–æ¥æ‚£è€…æ•°', 'ç·¯åº¦', 'çµŒåº¦', 'page', 'è¨ºç™‚ç§‘']

Get_ClinicId_BaseUrl = "https://iryojoho.pref.aichi.jp/medical/?dayofweek=&departmentcategoryid=10&languagelevel=%E2%97%8E&kenshin_keyword=&searchtype=function&gairai_keyword=&objecttype=1%2C2%2C4&requestpage="



###  Functions

## Functions  predefined
# -------------------------------------------------------------------------------------
def _normalization(arg):
    """
    æ–‡å­—åˆ—ã®æ­£è¦åŒ–ã‚’è¡Œã†å†…éƒ¨é–¢æ•°ã€‚
    ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«ã€å…¨è§’ã‚’åŠè§’ã«ã€å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›ã—ã€ä¸å¯è¦–æ–‡å­—ã‚‚å‰Šé™¤ã™ã‚‹ã€‚
    """

    try:
        # ã²ã‚‰ãŒãªã‚’ã‚«ã‚¿ã‚«ãƒŠã«å¤‰æ›
        try:
            result = jaconv.hira2kata(arg)
        except AttributeError:
            result = arg

        # å…¨è§’ã‚’åŠè§’ã«å¤‰æ›
        try:
            result = jaconv.z2h(result, digit=True, ascii=True)
        except AttributeError:
            result = result

        # å¤§æ–‡å­—ã‚’å°æ–‡å­—ã«å¤‰æ›
        try:
            result = result.lower()
        except AttributeError:
            result = result

        # ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤
        try:
            result = _str_clean(result)
        except TypeError:
            result = result

    except:
        result = arg

    return result

def normalization(arg):
    # print(2, arg)
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã‚’æ­£è¦åŒ–ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_normalization, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _str_clean(arg):
    """
    æ–‡å­—åˆ—ã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """

    try:
        result = arg.strip()
    except:
        result = arg

    try:
        result = re.sub(r"\r|\n|\r\n|\u3000|\t|ã€€| |,", " ", result)
    except TypeError:
        result = result

    return result

# -------------------------------------------------------------------------------------
def str_clean(arg):
    """
    æ–‡å­—åˆ—ã¾ãŸã¯æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã®ã‚¹ãƒšãƒ¼ã‚¹ã¨ä¸å¯è¦–æ–‡å­—ã‚’å‰Šé™¤ã™ã‚‹ã€‚
    """

    # å†…éƒ¨é–¢æ•°ã‚’NumPyã®ufuncã«å¤‰æ›
    _func = np.frompyfunc(_str_clean, 1, 1)

    # ãƒªã‚¹ãƒˆã‚’NumPyé…åˆ—ã«å¤‰æ›
    _list = np.array(arg, dtype="object")

    # çµæœã‚’å–å¾—
    result = _func(_list)

    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›
    result = result if type(result) == str else result.tolist() if type(result) == np.ndarray else "error"

    return result

# -------------------------------------------------------------------------------------
def _split_buildingName(arg):
    # print(1, arg)
    """
    å»ºç‰©åã‚’åˆ‡ã‚Šåˆ†ã‘ã‚‹å†…éƒ¨é–¢æ•°ã€‚
    """
    ## ãƒã‚¤ãƒ•ãƒ³ã®ä¸€èˆ¬åŒ–
    address = normalization(arg)
    hyphens = '-Ë—á…³á­¸â€â€‘â€’â€“â€”â€•âƒâ»âˆ’â–¬â”€â”â–ãƒ¼ã…¡ï¹˜ï¹£ï¼ï½°ğ„ğ†‘áš€'
    address = re.sub("|".join(hyphens), "-", address)
    address = re.sub(r"([ï½±-ï¾])(-)",r"\1ï½°", address)

    ## ä¸ç›®ã€ç•ªåœ°ã€å·ãªã©ã§ä½¿ã‚ã‚Œã‚‹æ¼¢å­—ã®å®šç¾©
    chome_poplist = ["ï¾‰åˆ‡","ç”ºç›®","åœ°å‰²","ä¸ç›®","ä¸","çµ„","ç•ªç”º","ç•ªåœ°","ç•ªç›®","ç•ª","å·å®¤","å·","è¡—åŒº","ç”»åœ°"]
    chome_popset = r"|".join(chome_poplist)
    chome_holdlist = ["æ¡æ±","æ¡è¥¿","æ¡å—","æ¡åŒ—","æ¡é€š","æ¡","æ±","è¥¿","å—","åŒ—"]
    chome_holdset = r"|".join(chome_holdlist)
    chome_alllist = chome_popset + chome_holdset
    chome_allset = r"|".join(chome_alllist)

    ## separate address
    result = re.findall(re.compile(f"(.*\d\[{chome_allset}\]*)|(\D+\[-\d\]+)|(.*)"), address)

    ## convert kanji into hyphen
    result = [[re.sub(f"(\d+)({chome_popset})", r"\1-", "".join(t)) for t in tl] for tl in result]

    ## concat all
    result = ["".join(t) for t in result]
    result = "".join(result)

    ## special case handling (1ï¾‰3 1åŒº1)
    result = re.sub(r"([^ï½±ï½°ï¾])(ï¾‰|ï½°)(\d)", r"\1-\3", result)
    result = re.sub(r"(\d)(åŒº)(\d)", r"\1-\3", result)
    result = re.sub("--", "-", result)

    ## separate into [japanese] + [number + hyphen] chunks
    result = re.findall(re.compile(f"(\D+[-\d]+[{chome_holdset}]*[-\d]+)|(\D+[-\d]+)|(.*)"), result)
    result = [t for t in ["".join(tl) for tl in result] if t != ""]
    # print(3, result)
    ## merge [number + hyphen] chunks
    try:
        result = [result[0]] + ["".join(result[1:])]
    except:
        result = result

    # 2åˆ—ç›®ãŒå˜ç‹¬ã€Œf, éšã€ã®ã¨ãã€1åˆ—ç›®ã®æœ«å°¾æ•°ã‚’2åˆ—ç›®ã¸ç§»å‹•
    if re.fullmatch(r"f|éš", result[1]):
        result[1] = "".join(re.compile(r"\d+$").findall(result[0])) + result[1]
        result[0] = re.sub(r"\d+$", "", result[0])

    # 2åˆ—ç›®ã§ã€éšæ•°ãŒç•ªåœ°ã¨çµåˆã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã¨ãã€éšæ•°ã‚’1æ¡ã¨ã¿ãªã—ã€æ®‹ã‚Šã®æ•°å­—ã‚’ç•ªåœ°ã¨ã—ã¦1åˆ—ç›®ã¸ç§»å‹•
    if (re.fullmatch(r"\D+", result[0]) or re.search(r"-$", result[0])) and re.match(r"(\d*)(\d)(f|éš)(\d*)", result[1]):
        result[1] = re.sub(r"(\d*)(\d)(f|éš)(\d*)", r"\1,\2\3\4", result[1])
        result[0] = result[0] + result[1][:result[1].find(",")]
        result[1] = result[1][result[1].find(",")+1:]

    # æœ«å°¾ã®ãƒã‚¤ãƒ•ãƒ³ã‚’å‰Šé™¤
    result[0] = re.sub(r"-+$", "", result[0])

    return result


def get_page(url):
    response = requests.get(url, timeout = 20)
    if response.status_code == 200:
        return response
    else:
        get_page(url)

def get_base_data(html):
    html_info = BeautifulSoup(html, 'lxml')
    timeStamp = datetime.date.today()

    baseData = {
        'timestamp': timeStamp
    }

    try:
        storename = html_info.find("caption", string="1. è¨ºç™‚æ‰€ã®åç§°").find_next_sibling('tr').find_next_sibling('tr').find('td').text.strip().replace(" ", "").replace("\n", "")
    except:
        storename = "na"
    
    try:
        updateDate = html_info.find('span', {'style': 'font-size:80%;'}).text.replace("æœ€çµ‚æ›´æ–°æ—¥ï¼š", "").replace("ï¼ˆ", "").replace("ï¼‰", "").replace("\n", "").replace(" ", "").replace("\r", "")
    except:
        updateDate = "na"
    
    try:
        founder_text = html_info.find("th", string="è¨ºç™‚æ‰€ã®åç§°").find_next_sibling('td').text
        if "æ³•äºº" in founder_text:
            founder_type = founder_text[:founder_text.index('æ³•äºº')].replace("\n", "").replace("\t", "") + "æ³•äºº"
        else :
            founder_type = "å€‹äºº"
    except:
        founder_type = "å€‹äºº"

    try:
        founder_name = html_info.find("caption", string="2. è¨ºç™‚æ‰€ã®é–‹è¨­è€…").find_next_sibling('tr').find_next_sibling('tr').find('td').text.strip().replace("\n", "").replace(" ", "")
    except:
        founder_name = "na"

    try:
        admin_name = html_info.find("caption", string="3. è¨ºç™‚æ‰€ã®ç®¡ç†è€…").find_next_sibling('tr').find_next_sibling('tr').find('td').text.strip().replace("\n", "").replace(" ", "")
    except:
        admin_name = "na"

    try:
        # address_original = html_info.find("th", string="ä½æ‰€").find_next_sibling('td').text.strip().replace("\n", "").replace(" ", "").replace("é³¥å–çœŒ","")
        address_original = html_info.find("caption", string="4. è¨ºç™‚æ‰€ã®æ‰€åœ¨åœ°").find_next_sibling('tr').find_next_sibling('tr').find_next_sibling('tr').find("td").text.strip().replace("\n", "").replace(" ", "")
    except:
        address_original = "na"
    try:
        storeAddressNormalize = "".join(list(normalize(address_original).values())[0:4])
        address_normalize_0 = _split_buildingName(storeAddressNormalize)[0]
        address_normalize_1 = _split_buildingName(storeAddressNormalize)[1]
    except:
        address_normalize_0 = address_normalize_1 = "na"

    try:
        general_dentistry = "["
        try:
            general_dentistry_list = ['1. æ­¯ç§‘é ˜åŸŸã®ä¸€æ¬¡è¨ºç™‚', '2. æˆäººã®æ­¯ç§‘çŸ¯æ­£æ²»ç™‚', '3. å”‡é¡å£è“‹è£‚ã®æ­¯ç§‘çŸ¯æ­£æ²»ç™‚', '4. é¡å¤‰å½¢ç—‡ã®æ­¯ç§‘çŸ¯æ­£æ²»ç™‚', '5. éšœãŒã„è€…ç­‰ã®æ­¯ç§‘æ²»ç™‚', '6. æ‘‚é£Ÿæ©Ÿèƒ½éšœå®³ã®æ²»ç™‚']
            for item in general_dentistry_list:
                gerneral_ths = html_info.find("caption", string="23. å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç–¾æ‚£åˆã¯æ²»ç™‚ã®å†…å®¹").find_parent('table', {'class': 'search_detail_table'}).find_all('th')
                for gerneral_th in gerneral_ths:
                    if gerneral_th.text.strip() == item:
                        td_text = gerneral_th.find_next_sibling('td').text.strip()
                        if "â—¯" in td_text or "â—‹" in td_text:
                            general_dentistry += (item + ", ")
        except:
            print('not found')
        general_dentistry += "]"
    except:
        general_dentistry = "na"

    try:
        oral_surgery = "["
        try:
            oral_surgery_list = ['1. åŸ‹ä¼æ­¯æŠœæ­¯', '2. é¡é–¢ç¯€ç—‡æ²»ç™‚', '3. é¡å¤‰å½¢ç—‡æ²»ç™‚', '4. é¡éª¨éª¨æŠ˜æ²»ç™‚', '5. å£å”‡ãƒ»èˆŒãƒ»å£è…”ç²˜è†œã®ç‚ç—‡ãƒ»å¤–å‚·ãƒ»è…«ç˜ã®æ²»ç™‚', '6. å”‡é¡å£è“‹è£‚æ²»ç™‚']
            for item in oral_surgery_list:
                gerneral_ths = html_info.find("caption", string="23. å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç–¾æ‚£åˆã¯æ²»ç™‚ã®å†…å®¹").find_parent('table', {'class': 'search_detail_table'}).find_all('th')
                for gerneral_th in gerneral_ths:
                    if gerneral_th.text.strip() == item:
                        td_text = gerneral_th.find_next_sibling('td').text.strip()
                        if "â—¯" in td_text or "â—‹" in td_text:
                            oral_surgery += (item + ", ")
        except:
            print('not found')
        oral_surgery += "]"
    except:
        oral_surgery = "na"


    try:
        has_structure = "["
        try:
            has_structure_list = ['é§è»Šå ´ã®æœ‰ç„¡', '1. æ‰‹è©±ã«ã‚ˆã‚‹å¯¾å¿œ', '2. æ–½è¨­å†…ã®æƒ…å ±ã®è¡¨ç¤º', '3. éŸ³å£°ã«ã‚ˆã‚‹æƒ…å ±ã®ä¼é”', '4. æ–½è¨­å†…ç‚¹å­—ãƒ–ãƒ­ãƒƒã‚¯ã®è¨­ç½®', '5. ç‚¹å­—ã«ã‚ˆã‚‹è¡¨ç¤º', 'æ–½è¨­ã®ãƒãƒªã‚¢ãƒ•ãƒªãƒ¼åŒ–ã®å®Ÿæ–½', 'è»Šæ¤…å­ç­‰åˆ©ç”¨è€…ç”¨é§è»Šæ–½è¨­ã®æœ‰ç„¡', 'å¤šæ©Ÿèƒ½ãƒˆã‚¤ãƒ¬ã®è¨­ç½®', '1. æ–½è¨­å†…ã«ãŠã‘ã‚‹å…¨é¢ç¦ç…™ã®å®Ÿæ–½', '2. å¥åº·å¢—é€²æ³•ç¬¬28æ¡ç¬¬13å·ã«è¦å®šã™ã‚‹ç‰¹å®šå±‹å¤–å–«ç…™å ´æ‰€ã®è¨­ç½®']
            for item in has_structure_list:
                gerneral_ths = html_info.find("caption", string="10. è¨ºç™‚æ‰€ã®é§è»Šå ´").find_parent('table', {'class': 'search_detail_table'}).find_all('th')
                gerneral_ths += html_info.find("caption", string="16. éšœãŒã„è€…ã«å¯¾ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹").find_parent('table', {'class': 'search_detail_table'}).find_all('th')
                gerneral_ths += html_info.find("caption", string="17. è»Šæ¤…å­ç­‰åˆ©ç”¨è€…ã«å¯¾ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹å†…å®¹").find_parent('table', {'class': 'search_detail_table'}).find_all('th')
                gerneral_ths += html_info.find("caption", string="18. å—å‹•å–«ç…™ã‚’é˜²æ­¢ã™ã‚‹ãŸã‚ã®æªç½®").find_parent('table', {'class': 'search_detail_table'}).find_all('th')
                for gerneral_th in gerneral_ths:
                    if gerneral_th.text.strip() == item:
                        td_text = gerneral_th.find_next_sibling('td').text.strip()
                        if "â—¯" in td_text or "â—‹" in td_text or "æœ‰" in td_text:
                            has_structure += (item + ", ")
        except:
            print('not found')
        has_structure += "]"
    except:
        has_structure = "na"
        
    try:
        anesthesia_treatment = "["
        try:
            anesthesia_treatment_list = ['1. éº»é…”ç§‘æ¨™æ¦œåŒ»ã«ã‚ˆã‚‹éº»é…”ï¼ˆéº»é…”ç®¡ç†ï¼‰', '2. å…¨èº«éº»é…”', '3. ç¡¬è†œå¤–éº»é…”', '4. è„Šæ¤éº»é…”', '5. ç¥çµŒãƒ–ãƒ­ãƒƒã‚¯']
            for item in anesthesia_treatment_list:
                gerneral_ths = html_info.find("caption", string="23. å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹ç–¾æ‚£åˆã¯æ²»ç™‚ã®å†…å®¹").find_parent('table', {'class': 'search_detail_table'}).find_all('th')
                for gerneral_th in gerneral_ths:
                    if gerneral_th.text.strip() == item:
                        td_text = gerneral_th.find_next_sibling('td').text.strip()
                        if "â—¯" in td_text or "â—‹" in td_text:
                            anesthesia_treatment += (item + ", ")
        except:
            print('not found')
        anesthesia_treatment += "]"
    except:
        anesthesia_treatment = "na"

    try:
        home_care = "["
        affiliate_check = "["
        affiliate_list = ['1. ç—…é™¢ã¨ã®é€£æº', '2. è¨ºç™‚æ‰€ã¨ã®é€£æº', '3. è¨ªå•çœ‹è­·ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã®é€£æº', '4. å±…å®…ä»‹è­·æ”¯æ´äº‹æ¥­æ‰€ã¨ã®é€£æº', '5. è–¬å±€ã¨ã®é€£æº']
        try:
            gerneral_tds = html_info.find("caption", string="26. å¯¾å¿œã™ã‚‹ã“ã¨ãŒã§ãã‚‹åœ¨å®…åŒ»ç™‚").find_parent('table', {'class': 'search_detail_table'}).find_all('td')
            for gerneral_td in gerneral_tds:
                td_text = gerneral_td.text.strip()
                if "â—¯" in td_text or "â—‹" in td_text:
                    item = gerneral_td.find_previous_sibling('th').text.strip()
                    if item in affiliate_list:
                        affiliate_check += (item + ", ")
                    else:
                        home_care += (item + ", ")
        except:
            print('not found')
        home_care += "]"
        affiliate_check += "]"
    except:
        home_care = "na"
        affiliate_check = "na"

    dentist = '-'
    dental_hygienist = '-'
    try:
        person_ths = html_info.find("caption", string="28.æ­¯ç§‘è¨ºç™‚æ‰€ã®äººå“¡é…ç½®(å¸¸å‹¤æ›ç®—å¾Œ) ").find_parent('table', {'class': 'search_detail_table'}).find_all('th')
        for person_th in person_ths:
            person_th_text = person_th.text.strip()
            if person_th_text == '2.æ­¯ç§‘åŒ»å¸«':
                dentist = person_th.find_next_sibling('td').text.strip()
                if dentist == '':
                    dentist = "-"
            elif person_th_text == '7.æ­¯ç§‘è¡›ç”Ÿå£«':
                dental_hygienist = person_th.find_next_sibling('td').text.strip()
                if dental_hygienist == '':
                    dental_hygienist = "-"
    except:
        dentist = "-"
        dental_hygienist = "-"

    dental_technician = "-"
    dental_assistant = "-"

    try:
        average_people_count = html_info.find("caption", string="6. å¤–æ¥æ‚£è€…æ•°").find_next_sibling('tr').find('td').text.strip().replace(" ", "").replace("\n", "")
    except:
        average_people_count = "na"


    baseData['storename'] = storename
    baseData['updateDate'] = updateDate
    baseData['founder_type'] = founder_type
    baseData['founder_name'] = founder_name
    baseData['admin_name'] = admin_name
    baseData['address_original'] = address_original
    baseData['address_normalize[0]'] = address_normalize_0
    baseData['address_normalize[1]'] = address_normalize_1
    baseData['general_dentistry'] = general_dentistry
    baseData['oral_surgery'] = oral_surgery
    baseData['pediatric_dentistry'] = "na"
    baseData['orthodontic_dentistry'] = "na"
    baseData['has_structure'] = has_structure
    baseData['anesthesia_treatment'] = anesthesia_treatment
    baseData['home_care'] = home_care
    baseData['affiliate_check'] = affiliate_check
    baseData['dentist'] = dentist + '|-|-'
    baseData['dental_technician'] = dental_technician + '|-|-'
    baseData['dental_assistant'] = dental_assistant + '|-|-'
    baseData['dental_hygienist'] = dental_hygienist + '|-|-'
    baseData['average_people_count'] = average_people_count
    baseData['longitude'] = "na"
    baseData['latitude'] = "na"
    baseData['medical_department'] = ["æ­¯ç§‘é ˜åŸŸ", "å£è…”å¤–ç§‘é ˜åŸŸ"]
    # print(baseData)

    return baseData






def init():
    f = open("urls.txt", "r")
    datetime_module = builtins.__import__('datetime')
    Today = datetime_module.date.today()

    csv_file_name = "tottori" + str(Today) + ".csv"
    csv_file = open(csv_file_name, 'a', newline="", encoding="utf-8", errors="replace")
    writer = csv.writer(csv_file)
    writer.writerow(Arg)

    for line in f:
        page = line.split(',')[0]
        url = line.split(',')[1]
        data = {}
        print(url)

        baseInfo = get_page(url)
        if baseInfo.status_code == 200:
            baseData = get_base_data(baseInfo.text)
        
        data=[
            baseData['timestamp'],
            baseData['storename'],
            baseData['address_original'],
            baseData['address_normalize[0]'],
            baseData['address_normalize[1]'],
            baseData['updateDate'],
            url,
            baseData['founder_type'],
            baseData['founder_name'],
            baseData['admin_name'],
            baseData['general_dentistry'],
            baseData['oral_surgery'],
            baseData['pediatric_dentistry'],
            baseData['orthodontic_dentistry'],
            baseData['has_structure'],
            baseData['anesthesia_treatment'],
            baseData['home_care'],
            baseData['affiliate_check'],
            baseData['dentist'],
            baseData['dental_technician'],
            baseData['dental_assistant'],
            baseData['dental_hygienist'],
            baseData['average_people_count'],
            baseData['longitude'],
            baseData['latitude'],
            page,
            baseData['medical_department']
        ]
        
        writer.writerow(data)



    # url = 'https://medinfo.pref.tottori.lg.jp/ComDisp/dental_03_view.php?ID=76'
    # baseInfo = get_page(url)
    # if baseInfo.status_code == 200:
    #     print(baseInfo.text)
    #     # baseData = get_base_data(baseInfo.text)
    
    # detail_file = open("detail.html", "ab")
    # detail_file.write(baseInfo.content)
    # detail_file.close()

init()
